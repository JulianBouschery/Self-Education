{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2520931e-1e13-469c-a136-08d2da016c2a",
   "metadata": {},
   "source": [
    "Zurück zum:\n",
    "- [Inhaltsverzeichnis](../../../_Inhaltsverzeichnis_Data_Analyses_.ipynb)\n",
    "- [Syllabus](../../../_Syllabus_PCED_.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b580cd9a-fa89-4571-b6fb-fa1eba8e89f8",
   "metadata": {},
   "source": [
    "Siehe auch:\n",
    "- [Umfragen](../Umfragen.ipynb)\n",
    "- [Interviews](../Interview.ipynb)\n",
    "- [API's](../API.ipynb)\n",
    "- [Datenerhebung](../_Data_Collection_.ipynb)\n",
    "- [Auslesen von Datenspeichern](../auslesen.ipynb)\n",
    "- [Stichproben](../Stichproben.ipynb)\n",
    "- [Recht und Ethik](../recht_ethik.ipynb)\n",
    "- [Anonymisierung](../Anonymisierung.ipynb)\n",
    "- [Einfluss des Data-Analysten auf die Geschäftsprozesse](../Einfluss.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d4e9c-687c-4435-ad34-77592292b217",
   "metadata": {},
   "source": [
    "Beispiele:\n",
    "- [BitCoin](web_scrap_bit_coin.ipynb)\n",
    "- [BeautifulSoup](web_scraping_beautiful_soup.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f570d-95e5-4180-9b27-2dd1492afd99",
   "metadata": {},
   "source": [
    "Web Scraping ist das automatisierte Extrahieren von Daten aus Webseiten. Es ermöglicht die Sammlung von Informationen, die online verfügbar sind, aber nicht als Rohdaten zum Download bereitgestellt werden. Mit Web Scraping lassen sich Inhalte, wie Tabellen, Texte, Bilder und andere Elemente, systematisch sammeln und speichern.\n",
    "\n",
    "# Wie funktioniert Web Scraping?\n",
    "\n",
    "Web Scraping funktioniert durch das Laden einer Webseite, das Analysieren ihrer Struktur und das Extrahieren der benötigten Daten. Webseiten werden in HTML geschrieben, und ein Scraper durchsucht das HTML-Dokument, um bestimmte Informationen zu extrahieren. Python-Bibliotheken wie `BeautifulSoup`, `Scrapy` und `Selenium` ermöglichen es, diese Prozesse zu automatisieren.\n",
    "\n",
    "## Theoretischer Hintergrund\n",
    "\n",
    "Eine Webseite besteht aus HTML, CSS und oft auch JavaScript. Beim Scraping geht es hauptsächlich darum, das HTML zu analysieren, um gezielt die Daten auszulesen. Jede HTML-Seite hat eine DOM-Struktur (Document Object Model), in der verschiedene Elemente wie Überschriften (`<h1>`), Absätze (`<p>`), Tabellen (`<table>`) und Links (`<a>`) angeordnet sind. Web Scraping extrahiert diese Elemente, indem es den HTML-Baum durchläuft und die relevanten Teile extrahiert.\n",
    "\n",
    "\n",
    "\n",
    "Die Abkürzungen in HTML-Tags stehen für bestimmte Elemente:\n",
    "\n",
    "- `<h1>`: „Heading 1“ – Dies ist ein Tag für die größte Überschrift auf einer Seite. Es gibt auch `<h2>`, `<h3>`, usw., um kleinere Überschriften zu definieren.\n",
    "- `<p>`: „Paragraph“ – Ein Tag für einen Absatz Text.\n",
    "- `<table>`: „Table“ – Ein Tag für Tabellenstrukturen, die aus Zeilen und Spalten bestehen.\n",
    "- `<a>`: „Anchor“ – Ein Tag für Links (Anker), die auf andere Seiten oder Ressourcen verweisen.\n",
    "\n",
    "Diese Tags sind Teil der HTML-Sprache, die die Struktur und den Inhalt einer Webseite beschreibt.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Techniken und Methoden des Web Scraping\n",
    "\n",
    "1. **HTML Parsing (z. B. mit BeautifulSoup)**:\n",
    "   - Analyse des HTML-Codes der Seite, um bestimmte Tags, Klassen oder Attribute auszulesen.\n",
    "   - Gut geeignet für Seiten mit statischen Inhalten.\n",
    "   - Bibliotheken: `BeautifulSoup`, `lxml`.\n",
    "\n",
    "2. **API Requests**:\n",
    "   - Einige Webseiten bieten APIs an, die speziell für den Datenzugriff entwickelt wurden.\n",
    "   - Hierbei wird direkt auf die Daten zugegriffen, ohne das Parsing von HTML.\n",
    "   - Dies ist die effizienteste Methode, wenn eine API vorhanden ist.\n",
    "\n",
    "3. **Browser Automation (z. B. mit Selenium)**:\n",
    "   - Wenn Webseiten dynamische Inhalte verwenden (z. B. durch JavaScript), können Browserautomatisierungstools wie `Selenium` eingesetzt werden, um die Seite zu laden und die Daten zu extrahieren.\n",
    "   - Gut geeignet für dynamische Webseiten.\n",
    "\n",
    "4. **Headless Browser Scraping**:\n",
    "   - Ein Webbrowser ohne Benutzeroberfläche wird verwendet, um Webseiten zu laden. `Puppeteer` oder `Selenium` kann verwendet werden, um diese zu automatisieren.\n",
    "   - Effizient bei Webseiten mit dynamischen Inhalten.\n",
    "\n",
    "5. **Scrapy (Framework)**:\n",
    "   - Eine leistungsfähige Bibliothek für das Scraping von großen Webseiten oder komplexen Datenmodellen.\n",
    "   - Unterstützt parallele Anfragen und ist effizient für große Mengen von Daten.\n",
    "\n",
    "## Häufige Fehler und Probleme\n",
    "\n",
    "1. **Rate Limiting**: Viele Webseiten begrenzen die Anzahl der Anfragen pro Zeiteinheit, um Überlastungen zu vermeiden.\n",
    "2. **IP-Blockierung**: Webseiten können Scraper blockieren, wenn sie ungewöhnlich viele Anfragen erkennen.\n",
    "3. **Captcha**: Einige Seiten verwenden Captchas, um automatisierte Zugriffe zu verhindern.\n",
    "4. **Dynamische Inhalte**: Inhalte, die durch JavaScript nachgeladen werden, sind oft nicht direkt über einfaches HTML-Parsen zugänglich.\n",
    "5. **Urheberrecht und Nutzungsbedingungen**: Viele Webseiten haben Regeln bezüglich der Verwendung ihrer Inhalte. Es ist wichtig, diese zu beachten.\n",
    "\n",
    "\n",
    "## Beispiel: Einfaches Scraping einer statischen Seite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb07f08-4f08-496c-b2c0-9bfeddcf9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL der Webseite\n",
    "url = 'https://example.com'\n",
    "\n",
    "# Sende Anfrage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse das HTML der Seite\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Finde alle Überschriften der Klasse 'title'\n",
    "titles = soup.find_all('h1', class_='title')\n",
    "\n",
    "# Gib die Überschriften aus\n",
    "for title in titles:\n",
    "    print(title.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e805ee2-4fd9-42bd-a6fe-ed029dcb12e8",
   "metadata": {},
   "source": [
    "\n",
    "### Import der Module\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "- **`import requests`**: Importiert das `requests`-Modul, das für HTTP-Anfragen verwendet wird. Es ermöglicht das Senden von GET- und POST-Anfragen an Webseiten und das Abrufen von Inhalten.\n",
    "- **`from bs4 import BeautifulSoup`**: Importiert die `BeautifulSoup`-Klasse aus dem `bs4` (BeautifulSoup4)-Modul, das für das Parsen und Analysieren von HTML- und XML-Daten verwendet wird.\n",
    "\n",
    "### Definieren der URL\n",
    "```python\n",
    "# URL der Webseite\n",
    "url = 'https://example.com'\n",
    "```\n",
    "- Hier wird die URL der Webseite, die du scrapen möchtest, in der Variablen `url` gespeichert. In diesem Fall ist es eine Platzhalter-URL (`example.com`).\n",
    "\n",
    "### Senden der Anfrage\n",
    "```python\n",
    "# Sende Anfrage\n",
    "response = requests.get(url)\n",
    "```\n",
    "- **`requests.get(url)`**: Sendet eine HTTP GET-Anfrage an die angegebene URL. Diese Methode versucht, den HTML-Inhalt der Webseite abzurufen.\n",
    "- **`response`**: Die Antwort des Servers wird in der Variablen `response` gespeichert. Diese Antwort enthält den Statuscode (z. B. 200 für Erfolg) und den Inhalt der Webseite.\n",
    "\n",
    "### Parsen des HTML\n",
    "```python\n",
    "# Parse das HTML der Seite\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "```\n",
    "- **`response.text`**: Dies gibt den HTML-Inhalt der abgerufenen Webseite als String zurück.\n",
    "- **`BeautifulSoup(response.text, 'html.parser')`**: Erstellt ein `BeautifulSoup`-Objekt, das den HTML-Inhalt analysiert. \n",
    "  - **`'html.parser'`**: Dies ist der Parser, der verwendet wird, um den HTML-Inhalt zu verarbeiten. In diesem Fall ist es der integrierte HTML-Parser von Python.\n",
    "\n",
    "### Finden der Überschriften\n",
    "```python\n",
    "# Finde alle Überschriften der Klasse 'title'\n",
    "titles = soup.find_all('h1', class_='title')\n",
    "```\n",
    "- **`soup.find_all('h1', class_='title')`**: Diese Methode sucht nach allen `<h1>`-Tags im geparsten HTML, die die Klasse `title` haben.\n",
    "  - **`'h1'`**: Der Tag, den du suchst (in diesem Fall eine Überschrift der ersten Ebene).\n",
    "  - **`class_='title'`**: Dies ist ein Filter, um nur die Elemente mit der Klasse `title` zu finden. (Beachte, dass du möglicherweise eine tatsächliche Klasse verwenden musst, die auf der Zielseite existiert.)\n",
    "- Das Ergebnis wird in der Liste `titles` gespeichert, die alle gefundenen `<h1>`-Elemente enthält.\n",
    "\n",
    "### Ausgeben der Überschriften\n",
    "```python\n",
    "# Gib die Überschriften aus\n",
    "for title in titles:\n",
    "    print(title.text)\n",
    "```\n",
    "- **`for title in titles:`**: Iteriert über jedes Element in der Liste `titles`.\n",
    "- **`print(title.text)`**: Gibt den Textinhalt jedes gefundenen `<h1>`-Tags aus. Die `.text`-Methode extrahiert nur den sichtbaren Text ohne HTML-Tags.\n",
    "\n",
    "### Zusammenfassung des Ablaufs\n",
    "1. Das Skript importiert die benötigten Module.\n",
    "2. Es definiert die URL der Webseite, die gescrapt werden soll.\n",
    "3. Eine GET-Anfrage wird an diese URL gesendet, um den HTML-Inhalt abzurufen.\n",
    "4. Der HTML-Inhalt wird mit BeautifulSoup geparst.\n",
    "5. Das Skript sucht nach allen `<h1>`-Tags mit der Klasse `title` und speichert sie in einer Liste.\n",
    "6. Schließlich wird der Text dieser Überschriften ausgegeben.\n",
    "\n",
    "### Wichtige Punkte\n",
    "- **Error Handling**: Der Code enthält kein Fehlerhandling (z. B. für 404-Fehler oder andere HTTP-Statuscodes). Es wäre ratsam, dies hinzuzufügen, um Probleme bei der Verbindung zur Webseite zu erkennen.\n",
    "- **Robots.txt und rechtliche Aspekte**: Bevor du mit dem Scraping beginnst, solltest du die `robots.txt`-Datei der Webseite überprüfen und die rechtlichen Bestimmungen beachten, um sicherzustellen, dass das Scraping zulässig ist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071839f-c1fb-421b-bea0-5e78db80b40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a700074d-a0f6-48df-a806-7c7f678b8d87",
   "metadata": {},
   "source": [
    "## Unterschiede zwischen den Methoden\n",
    "\n",
    "- **HTML Parsing** ist effizient, wenn die Inhalte statisch und gut strukturiert sind.\n",
    "- **API Requests** bieten direkte Datenzugriffe, sind jedoch nur verfügbar, wenn eine API bereitgestellt wird.\n",
    "- **Browser Automation** eignet sich für dynamische Seiten, die mit JavaScript arbeiten.\n",
    "- **Headless Browsers** bieten ähnliche Vorteile wie Browser Automation, sind aber ressourcenschonender.\n",
    "\n",
    "## Worauf muss man achten?\n",
    "\n",
    "- **Nutzungsbedingungen**: Viele Webseiten erlauben das Scraping nicht. Überprüfe immer die `robots.txt` und die Nutzungsbedingungen.\n",
    "- **Respektiere die Serverkapazitäten**: Sende nicht zu viele Anfragen in kurzer Zeit (Rate Limiting).\n",
    "- **IP-Blockierung vermeiden**: Nutze Rotationen von User-Agents oder Proxies, um IP-Blockierungen zu umgehen.\n",
    "\n",
    "## Spezialfälle\n",
    "\n",
    "- **Captcha-Handling**: Mit Tools wie `Selenium` oder speziellen Diensten lässt sich dieses Problem umgehen.\n",
    "- **AJAX-Inhalte**: Dynamisch nachgeladene Inhalte können durch Browser Automation oder durch Nachahmung der API-Anfragen geladen werden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b081624-4da5-43b1-9c87-c17d058f7d00",
   "metadata": {},
   "source": [
    "## Beispiel API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd868a-f1e1-4643-823e-fec8630773c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lösung Web-Scraping\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "# Schritt 1: Daten von einer API-Datenquelle abrufen (Random User API)\n",
    "url = 'https://randomuser.me/api/?results=10'  # Abrufen von 10 zufälligen Benutzern\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Schritt 2: Die relevanten Daten extrahieren\n",
    "users_data = []\n",
    "for user in data['results']:\n",
    "    user_info = {\n",
    "        'Name': f\"{user['name']['first']} {user['name']['last']}\",\n",
    "        'Geschlecht': user['gender'],\n",
    "        'Email': user['email'],\n",
    "        'Land': user['location']['country'],\n",
    "        'Telefonnummer': user['phone']\n",
    "    }\n",
    "    users_data.append(user_info)\n",
    "\n",
    "# Schritt 3: Anonymisierung der Daten\n",
    "fake = Faker()  # Faker-Bibliothek zur Erstellung zufälliger, gefälschter Daten\n",
    "anonymized_data = []\n",
    "for user in users_data:\n",
    "    anonymized_user = {\n",
    "        'Anonymisierter Name': fake.name(),\n",
    "        'Geschlecht': user['Geschlecht'],  # Behalten wir bei\n",
    "        'Anonymisierte Email': fake.email(),\n",
    "        'Anonymisiertes Land': fake.country(),\n",
    "        'Anonymisierte Telefonnummer': fake.phone_number()\n",
    "    }\n",
    "    anonymized_data.append(anonymized_user)\n",
    "\n",
    "# Schritt 4: In ein DataFrame umwandeln und anzeigen\n",
    "df_anonymized = pd.DataFrame(anonymized_data)\n",
    "print(\"Anonymisierte Daten:\")\n",
    "print(df_anonymized)\n",
    "\n",
    "# Optional: Daten in eine CSV-Datei speichern\n",
    "df_anonymized.to_csv('anonymized_users.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb1bd8e-1165-498e-be0d-83694af34035",
   "metadata": {},
   "source": [
    "---\n",
    "# Pagination\n",
    "\n",
    "Pagination ist ein wichtiger Aspekt beim Web Scraping, insbesondere wenn Daten auf mehreren Seiten einer Webseite verteilt sind. Webseiten mit vielen Inhalten wie Produktlisten, Nachrichtenartikeln oder Foren verwenden oft Pagination, um die Inhalte auf mehrere Seiten zu verteilen.\n",
    "\n",
    "## Was ist Pagination?\n",
    "\n",
    "Pagination bezeichnet die Praxis, große Mengen an Inhalten auf mehrere Seiten aufzuteilen, sodass die Benutzer nicht alles auf einer Seite sehen. Dies wird oft durch Links wie „Nächste Seite“ oder durch Nummerierungen („Seite 1, Seite 2, …“) realisiert. Beim Scraping muss man daher nicht nur die Daten einer Seite extrahieren, sondern auch systematisch durch die verschiedenen Seiten navigieren.\n",
    "\n",
    "## Wie funktioniert Scraping mit Pagination?\n",
    "\n",
    "Beim Scraping von paginierten Inhalten ist es wichtig, den Mechanismus zu verstehen, wie eine Seite zur nächsten übergeht. Dies kann auf verschiedene Arten geschehen:\n",
    "\n",
    "1. **URL-basiertes Paging**: Die URL enthält eine Parameteränderung für jede neue Seite, z. B. `?page=2`.\n",
    "2. **Link-basiertes Paging**: Es gibt Links auf der Webseite wie „Nächste Seite“ oder eine Liste von Seitenzahlen, auf die man klicken muss.\n",
    "3. **JavaScript-basierte Pagination**: Dynamische Webseiten laden neue Inhalte mittels AJAX (ohne Neuladen der Seite).\n",
    "\n",
    "## Beispiel 1: Scraping mit URL-basierter Pagination (BeautifulSoup)\n",
    "\n",
    "Hier wird die Seitennummer als URL-Parameter verwendet, was den Scraping-Prozess erleichtert:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Basis-URL ohne Seitenparameter\n",
    "base_url = 'https://example.com/products?page='\n",
    "\n",
    "# Definiere die Anzahl der Seiten (z.B. 5 Seiten)\n",
    "for page in range(1, 6):\n",
    "    # Konstruiere die URL für jede Seite\n",
    "    url = base_url + str(page)\n",
    "    \n",
    "    # Sende die Anfrage\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse den HTML-Inhalt\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extrahiere die gewünschten Daten\n",
    "    products = soup.find_all('div', class_='product-name')\n",
    "    for product in products:\n",
    "        print(product.text)\n",
    "```\n",
    "\n",
    "## Beispiel 2: Scraping mit Link-basierter Pagination (Selenium)\n",
    "\n",
    "Wenn es einen „Nächste Seite“-Button oder andere Links für die Navigation gibt, kannst du Selenium verwenden, um die Links zu klicken:\n",
    "\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Öffne den Browser\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Lade die erste Seite\n",
    "driver.get('https://example.com/products')\n",
    "\n",
    "# Extrahiere Daten von der ersten Seite\n",
    "products = driver.find_elements(By.CLASS_NAME, 'product-name')\n",
    "for product in products:\n",
    "    print(product.text)\n",
    "\n",
    "# Durchlaufe die Seiten, solange der \"Nächste\"-Button vorhanden ist\n",
    "while True:\n",
    "    try:\n",
    "        # Finde und klicke auf den \"Nächste Seite\"-Button\n",
    "        next_button = driver.find_element(By.LINK_TEXT, 'Nächste')\n",
    "        next_button.click()\n",
    "        \n",
    "        # Extrahiere die Produkte der nächsten Seite\n",
    "        products = driver.find_elements(By.CLASS_NAME, 'product-name')\n",
    "        for product in products:\n",
    "            print(product.text)\n",
    "    except:\n",
    "        # Wenn kein \"Nächste\"-Button mehr vorhanden ist, breche die Schleife ab\n",
    "        print(\"Keine weiteren Seiten.\")\n",
    "        break\n",
    "\n",
    "# Schließe den Browser\n",
    "driver.quit()\n",
    "```\n",
    "\n",
    "## Wichtige Aspekte beim Scraping mit Pagination:\n",
    "\n",
    "1. **URL-Änderungen überwachen**: Prüfe, ob sich die URL zwischen den Seiten ändert, um die Struktur der Seite zu verstehen.\n",
    "2. **Ende der Seiten erkennen**: Du solltest in der Lage sein, zu erkennen, wann du die letzte Seite erreicht hast. Dies könnte daran erkennbar sein, dass kein „Nächste Seite“-Link mehr existiert oder die Seitennummerierung aufhört.\n",
    "3. **Datenvalidierung**: Manchmal sind bestimmte Seiten leer oder enthalten Fehler. Es ist wichtig, Mechanismen einzubauen, um fehlerhafte Daten oder leere Seiten zu erkennen.\n",
    "\n",
    "## Verschiedene Pagination-Methoden und ihre Herausforderungen\n",
    "\n",
    "1. **Statische URLs** (wie im Beispiel oben): Am einfachsten zu handhaben, da du nur die URL anpassen musst.\n",
    "2. **JavaScript-basierte Pagination (AJAX)**: Hier lädt JavaScript die neuen Inhalte nach. In solchen Fällen kann Selenium oder das direkte Simulieren von AJAX-Anfragen nötig sein.\n",
    "3. **Unendliches Scrollen**: Viele moderne Seiten verwenden unendliches Scrollen, bei dem die Seite automatisch mehr Inhalte lädt, wenn der Benutzer nach unten scrollt. Auch hier ist `Selenium` oder ein `Headless Browser` notwendig.\n",
    "\n",
    "## Häufige Fehler beim Pagination-Scraping\n",
    "\n",
    "1. **Falsche Seitennavigation**: Wenn der Scraper nicht korrekt von Seite zu Seite navigiert, können Seiten übersprungen oder doppelt verarbeitet werden.\n",
    "2. **Timeouts und Ladefehler**: Manchmal laden Seiten nicht schnell genug oder es kommt zu Ladefehlern, insbesondere bei vielen Anfragen.\n",
    "3. **Rate Limiting/Blocking**: Bei vielen Seitenanfragen kann der Server deine IP blockieren.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c5c00-c9a7-42b6-a5ec-6addb337b8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "facda13f-b09d-4f8b-b85b-df1ad6bc425e",
   "metadata": {},
   "source": [
    "# Statische und dynamische Inhalte\n",
    "Der Unterschied zwischen **statischen** und **dynamischen** Inhalten liegt in der Art, wie die Daten auf einer Webseite bereitgestellt und angezeigt werden.\n",
    "\n",
    "## Statische Inhalte\n",
    "Statische Inhalte sind fix und ändern sich nicht, wenn die Seite geladen wird. Sie werden direkt in HTML auf dem Server erstellt und bleiben für alle Besucher gleich. Der Server sendet dieselben Daten an jeden Benutzer, ohne dass diese durch Benutzereingaben oder andere Faktoren beeinflusst werden.\n",
    "\n",
    "### Eigenschaften:\n",
    "- **Festgelegt**: Einmal erstellt, bleiben sie unverändert, bis der Entwickler sie manuell ändert.\n",
    "- **Einfach zu scrapen**: Da der HTML-Code die Inhalte direkt enthält, können sie leicht extrahiert werden.\n",
    "- **Schneller zu laden**: Da keine zusätzlichen Anfragen oder Verarbeitungen nötig sind, laden sie schneller.\n",
    "\n",
    "### Beispiele für statische Inhalte:\n",
    "- Eine einfache Informationsseite mit Text und Bildern (z. B. eine „Über uns“-Seite).\n",
    "- Statische Produktseiten, die immer dieselben Informationen anzeigen (z. B. eine Produktübersicht).\n",
    "- Blogbeiträge oder Nachrichtenartikel, die nach dem Veröffentlichen nicht mehr geändert werden.\n",
    "\n",
    "### Web Scraping bei statischen Inhalten:\n",
    "Für statische Inhalte kann man HTML-Parser wie **BeautifulSoup** verwenden, um den HTML-Code direkt zu durchsuchen und die gewünschten Daten zu extrahieren.\n",
    "\n",
    "## Dynamische Inhalte\n",
    "Dynamische Inhalte ändern sich in Abhängigkeit von Benutzereingaben, Interaktionen oder Echtzeitdaten. Sie werden oft durch Skriptsprachen wie JavaScript oder durch serverseitige Technologien (z. B. PHP, Python) generiert und nachträglich geladen. Wenn ein Benutzer eine Webseite besucht, wird der Inhalt je nach den aktuellen Bedingungen, Benutzereinstellungen oder anderen Faktoren erstellt.\n",
    "\n",
    "### Eigenschaften:\n",
    "- **Veränderbar**: Die angezeigten Inhalte ändern sich, z. B. durch Benutzereingaben oder Zeitereignisse.\n",
    "- **Schwerer zu scrapen**: Da der Inhalt oft durch JavaScript nachgeladen wird, kann ein einfacher HTML-Parser nicht die vollständigen Daten extrahieren. Es muss ein Tool wie **Selenium** oder ein Headless-Browser verwendet werden, um den dynamischen Inhalt zu laden.\n",
    "- **Interaktiv**: Diese Seiten sind oft benutzerfreundlicher und reagieren auf Nutzeraktionen (z. B. durch Filter, Suchanfragen oder interaktive Elemente).\n",
    "\n",
    "### Beispiele für dynamische Inhalte:\n",
    "- **Soziale Netzwerke**: Der Inhalt von Facebook oder Twitter ändert sich ständig und wird durch Benutzereingaben (Posts, Likes) und Updates dynamisch geladen.\n",
    "- **Produktsuchseiten**: Online-Shops wie Amazon passen die Inhalte basierend auf Suchfiltern, Sortierungen und Benutzervorlieben an.\n",
    "- **Live-Daten**: Webseiten mit Live-Updates, wie Wetterseiten, Aktienkurse oder Sportergebnisse, bei denen sich der Inhalt regelmäßig ändert, ohne die Seite neu zu laden.\n",
    "- **Infinite Scroll**: Seiten, bei denen Inhalte automatisch nachgeladen werden, wenn der Benutzer nach unten scrollt, wie bei sozialen Netzwerken oder Nachrichtenseiten.\n",
    "\n",
    "### Web Scraping bei dynamischen Inhalten:\n",
    "Für dynamische Inhalte ist es notwendig, Tools zu verwenden, die JavaScript interpretieren können, wie **Selenium**, oder die Netzwerkanfragen direkt zu analysieren (z. B. durch Nachahmung von API-Anfragen).\n",
    "\n",
    "## Zusammenfassung der Unterschiede:\n",
    "\n",
    "| Merkmal              | Statische Inhalte                         | Dynamische Inhalte                         |\n",
    "|----------------------|-------------------------------------------|--------------------------------------------|\n",
    "| **Datenquelle**       | Direkt in HTML vorhanden                  | Werden per JavaScript oder Servergenerierung nachgeladen |\n",
    "| **Veränderbarkeit**   | Fix und unveränderlich                    | Ändert sich basierend auf Benutzereingaben oder Zeitereignissen |\n",
    "| **Scraping**          | Einfach zu scrapen (mit BeautifulSoup)    | Schwerer zu scrapen (mit Selenium oder API-Anfragen) |\n",
    "| **Beispiele**         | „Über uns“-Seiten, Blogartikel            | Soziale Netzwerke, Online-Shops, Live-Daten |\n",
    "\n",
    "Diese Unterscheidung ist wichtig, um die richtige Scraping-Technik zu wählen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155acfb9-222c-4cf1-9013-4602c8693862",
   "metadata": {},
   "source": [
    "# Praktische Aufgabe: Zitate scrapen\n",
    "\n",
    "Praktische Aufgabe: Scrapen Sie die Seite http://quotes.toscrape.com/ und fragen Sie nach einem Zitat, was \"life\" enthält\n",
    "\n",
    "## Einfache Anfrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a8a61-ef77-499c-b1d9-ffbec1727b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einfache Anfrage\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL der Webseite mit Zitaten\n",
    "url = \"http://quotes.toscrape.com/\"\n",
    "\n",
    "# Anfrage an die Webseite senden\n",
    "response = requests.get(url)\n",
    "\n",
    "# Überprüfen, ob die Anfrage erfolgreich war\n",
    "if response.status_code == 200:\n",
    "    print(\"Anfrage erfolgreich!\")\n",
    "else:\n",
    "    print(f\"Fehler bei der Anfrage: {response.status_code}\")\n",
    "\n",
    "# HTML-Inhalt der Webseite parsen\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Zitate und Autoren auf der Seite finden\n",
    "quotes = soup.find_all('div', class_='quote')\n",
    "\n",
    "# Anzeigen der Zitate und Autoren\n",
    "for i, quote in enumerate(quotes, 1):\n",
    "    text = quote.find('span', class_='text').text\n",
    "    author = quote.find('small', class_='author').text\n",
    "    print(f\"{i}. {text} - {author}\")\n",
    "\n",
    "\n",
    "print(\"Anzeige mit Life\")\n",
    "# Anzeigen der Zitate und Autoren, die das Wort \"animal\" enthalten\n",
    "for i, quote in enumerate(quotes, 1):\n",
    "    text = quote.find('span', class_='text').text\n",
    "    author = quote.find('small', class_='author').text\n",
    "    if \"life\" in text.lower():  # Sucht nach dem Wort \"animal\" im Zitat\n",
    "        print(f\"{i}. {text} - {author}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e343d-bcfd-45a5-905f-4b0bb6732385",
   "metadata": {},
   "source": [
    "## Umfangreicher über alle Seiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce216f3-98b8-4182-946c-29ee633ad396",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Start-URL der Webseite\n",
    "base_url = \"http://quotes.toscrape.com/page/{}/\"\n",
    "page = 1\n",
    "found_quotes = []\n",
    "\n",
    "# Schleife durch Seiten, bis mindestens 20 Zitate gefunden wurden\n",
    "while len(found_quotes) < 2:\n",
    "    # Anfrage an die aktuelle Seite senden\n",
    "    response = requests.get(base_url.format(page))\n",
    "\n",
    "    # Überprüfen, ob die Anfrage erfolgreich war\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Fehler bei der Anfrage auf Seite {page}: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    # HTML-Inhalt der Seite parsen\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Zitate auf der Seite finden\n",
    "    quotes = soup.find_all('div', class_='quote')\n",
    "\n",
    "    # Zitate sammeln, die das Wort \"life\" enthalten\n",
    "    for quote in quotes:\n",
    "        text = quote.find('span', class_='text').text\n",
    "        author = quote.find('small', class_='author').text\n",
    "        if \"life\" in text.lower():\n",
    "            found_quotes.append(f\"{len(found_quotes) + 1}. {text} - {author}\")\n",
    "        \n",
    "        # Wenn wir mindestens 20 Zitate gefunden haben, abbrechen\n",
    "        if len(found_quotes) >= 5:\n",
    "            break\n",
    "\n",
    "    # Zur nächsten Seite wechseln\n",
    "    page += 1\n",
    "\n",
    "# Gefundene Zitate anzeigen\n",
    "for quote in found_quotes:\n",
    "    print(quote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1dc49-52ec-40f6-8bf1-079d04e6f934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
