{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b498c16-1055-44a6-8460-859fd477e75b",
   "metadata": {},
   "source": [
    "# Warum benötigt man die Normalisierung?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810f5975-2f2e-4ca2-bb99-5ccb1d937fe8",
   "metadata": {},
   "source": [
    "Um den Prozess der Normalisierung allgemein zu verstehen und die Probleme unnormalisierten Daten zu verdeutlichen, schauen wir uns ein Beispiel mit einem kleinen Datensatz an. Nehmen wir an, wir haben eine einfache Tabelle mit den Ergebnissen von zwei verschiedenen Tests für eine Gruppe von Schülern. Der Datensatz sieht folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e8ccc1-9af7-4a16-9faf-450181b5c98f",
   "metadata": {},
   "source": [
    "### Unnormalisierte Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cf6266-45eb-4177-a529-39e493a3bb82",
   "metadata": {},
   "source": [
    "| Schüler | Test 1 | Test 2 |\n",
    "|---------|--------|--------|\n",
    "| A       | 85     | 30     |\n",
    "| B       | 95     | 60     |\n",
    "| C       | 75     | 10     |\n",
    "| D       | 80     | 50     |\n",
    "| E       | 90     | 20     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce15e0f-7463-4b37-be6c-d2de54f38230",
   "metadata": {},
   "outputs": [],
   "source": [
    "Haben die Schüler im zweiten Test jetzt gut oder schlecht abgeschnitten?\n",
    "\n",
    "Was ist die gemeinsame Einheit? Prozent? Punkte? Wen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443ecb2-d8f1-4150-92ee-8a3ba9bc5b6b",
   "metadata": {},
   "source": [
    "#### Probleme mit unnormalisierten Daten\n",
    "\n",
    "1. **Unterschiedliche Skalen**: \n",
    "   - In diesem Beispiel hat Test 1 Werte zwischen 75 und 95, während Test 2 Werte zwischen 10 und 60 hat. Diese unterschiedlichen Skalen können die Analyse und das Training von Machine-Learning-Modellen beeinflussen. Wenn ein Algorithmus die Rohwerte verwendet, könnte er Test 1 als viel wichtiger erachten, da die Werte im Vergleich zu Test 2 höher sind.\n",
    "\n",
    "2. **Schwierigkeiten bei der Vergleichbarkeit**:\n",
    "   - Ohne Normalisierung ist es schwierig, die Ergebnisse zwischen verschiedenen Tests zu vergleichen. Zum Beispiel, ein Schüler, der in Test 1 85 Punkte hat, könnte in Test 2 eine ganz andere Punktzahl haben, die nicht einfach zu interpretieren ist.\n",
    "\n",
    "3. **Sensitivität von Modellen**:\n",
    "   - Viele Machine-Learning-Algorithmen, wie z.B. K-Means-Clustering oder k-Nearest Neighbors, sind empfindlich gegenüber der Skala der Daten. Unnormalisierte Daten könnten dazu führen, dass einige Features (hier die Testergebnisse) mehr Gewicht erhalten als andere, was die Ergebnisse verfälschen kann.\n",
    "\n",
    "### Min-Max-Normalisierung\n",
    "\n",
    "Um diese Probleme zu adressieren, verwenden wir die Min-Max-Normalisierung. Der Prozess sieht so aus:\n",
    "\n",
    "1. Bestimme den Minimal- und Maximalwert jeder Spalte.\n",
    "2. Wende die Min-Max-Normalisierung an, um jeden Wert \\(x\\) mit der Formel zu transformieren:\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)}\n",
    "$$\n",
    "\n",
    "Lass uns nun die Min-Max-Normalisierung auf unseren Datensatz anwenden.\n",
    "\n",
    "### Anwendung der Min-Max-Normalisierung\n",
    "\n",
    "Hier ist der Python-Code zur Normalisierung der obigen Daten mit NumPy und scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdeb3cc6-69a1-4b93-962a-2b2ca994a621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max normalisierte Daten:\n",
      "[[0.5  0.4 ]\n",
      " [1.   1.  ]\n",
      " [0.   0.  ]\n",
      " [0.25 0.8 ]\n",
      " [0.75 0.2 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Unnormalisierte Daten\n",
    "data = np.array([\n",
    "    [85, 30],\n",
    "    [95, 60],\n",
    "    [75, 10],\n",
    "    [80, 50],\n",
    "    [90, 20]\n",
    "])\n",
    "\n",
    "# Min-Max-Skalierung anwenden\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Min-Max normalisierte Daten:\")\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1701a-24af-4f62-b1e8-25ae5fd0bc3c",
   "metadata": {},
   "source": [
    "Jetzt sind die Werte zwischen 0 und 1 normalisiert, und du kannst sie besser vergleichen und analysieren, ohne dass eine Spalte die andere übermäßig beeinflusst. \n",
    "\n",
    "### Fazit\n",
    "\n",
    "Die Min-Max-Normalisierung hilft dabei, die Probleme unnormalisierter Daten zu beheben, indem sie eine einheitliche Skala für die Daten schafft, was die Vergleichbarkeit und die Leistung von Machine-Learning-Modellen verbessert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9c19e-0cd9-460e-85a9-c1482815d24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b10e304-b871-457a-a233-b63deaf89718",
   "metadata": {},
   "source": [
    "# Wann ist Normalisierung notwendig?\n",
    "\n",
    "- bei Daten mit unterschiedlicher Größenordnung\n",
    "  - z.B. Alter im Bereich 20-60, Einkommen im Bereich 30.000-100.000...\n",
    "- wenn Algorithmen wie k-nearest neighbors (k-NN), logistische Regression oder neuronale Netze verwendet werden\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96d60c-0fbb-4ced-9498-443bf1e3d756",
   "metadata": {},
   "source": [
    "# Einfluss auf das Machine Learning\n",
    "\n",
    "- **Skalierung der Daten** beeinflusst Leistung von maschinellen Lernmodellen erheblich\n",
    "- v.a. in Algorithmen, die auf Distanzen oder Gewichten basieren\n",
    "\n",
    "<br>\n",
    "\n",
    "**1. Verbesserte Genauigkeit bei distanzbasierten Algorithmen** \n",
    "- Algorithmen wie **K-Nearest Neighbors (KNN)**, **Support Vector Machines (SVM)** und **k-Means** verwenden Distanzen zwischen den Datenpunkten. \n",
    "- ohne Skalierung könnten Merkmale mit großen Werten den Abstand dominieren und die kleineren Merkmale \"überstimmen\", obwohl sie genauso wichtig sein könnten\n",
    "\n",
    "- **Ohne Skalierung**: Merkmale mit größeren Werten beeinflussen den Algorithmus mehr, was zu verzerrten Ergebnissen führt\n",
    "- **Mit Skalierung**: Alle Merkmale haben den gleichen Einfluss, was zu einer besseren Modellleistung führt.\n",
    "<br>\n",
    "\n",
    "**Beispiel:**\n",
    "Ein Datensatz enthält zwei Merkmale: Alter (0–100) und Einkommen (0–100,000). <br>\n",
    "Ohne Skalierung würde das Einkommen das Alter dominieren, was den Algorithmus in die falsche Richtung lenken könnte.\n",
    "\n",
    "<br>\n",
    "\n",
    "**2. Schnellere und stabilere Konvergenz bei gradientenbasierten Algorithmen**\n",
    "- Algorithmen mit **Gradientenabstieg** verwenden profitieren stark von der Skalierung\n",
    "- z.B. **lineare Regression**, **logistische Regression** oder **neuronale Netze**\n",
    "- bei nicht skalierten Daten kann der Gradientenabstieg ineffizient und langsam konvergieren, da die Schrittweiten für Merkmale unterschiedlicher Skalen stark variieren\n",
    "\n",
    "- **Ohne Skalierung**: Der Gradientenabstieg macht für Merkmale mit größeren Werten größere Schritte, was zu einer langsamen oder unstabilen Konvergenz führt.\n",
    "- **Mit Skalierung**: Die Schritte des Gradientenabstiegs sind gleichmäßig über alle Merkmale, was zu einer schnelleren und stabileren Konvergenz führt.\n",
    "\n",
    "<br>\n",
    "\n",
    "**3. Verbesserte Modellinterpretation**\n",
    "- in einigen Algorithmen geben die Koeffizienten die relative Bedeutung der Merkmale an\n",
    "\n",
    "- nicht-skalierte Daten können die Interpretation der Bedeutung der Koeffizienten erschweren, da sie von der Skala der Variablen abhängen\n",
    "\n",
    "- **Ohne Skalierung**: Die Koeffizienten können irreführend oder schwer zu interpretieren sein.\n",
    "- **Mit Skalierung**: Die Koeffizienten zeigen den tatsächlichen Einfluss jedes Merkmals, unabhängig von dessen ursprünglicher Skala.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Wann ist Skalierung nicht so entscheidend?\n",
    "- bei Modellen wie **Baum-basierten Algorithmen** (z.B. **Entscheidungsbäume**, **Random Forests**) i.d.R. nicht erforderlich\n",
    "- diese Modelle sind von Splits auf den Merkmalen abhängig sind und nicht von Distanzen oder Koeffizienten\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc97d5-a5c0-43e4-a000-8eafece9b15e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Welche Probleme können auftreten, wenn Variablen in verschiedenen Maßstäben vorliegen?\n",
    "\n",
    "\n",
    "1. **Dominanz großer Skalen**: <br>\n",
    "Variablen mit größeren Werten dominieren das Modell und überlagern kleinere, auch wenn diese wichtig sind.\n",
    "   \n",
    "2. **Verzerrte Distanzberechnungen**: <br>\n",
    "Bei distanzbasierten Algorithmen wie KNN beeinflussen Variablen mit größeren Werten die Distanzen unverhältnismäßig stark.\n",
    "\n",
    "3. **Langsame Konvergenz**: <br>\n",
    "Gradientbasierte Algorithmen (z.B. lineare Regression) konvergieren langsamer oder instabil, wenn die Variablen unterschiedlich skaliert sind.\n",
    "\n",
    "4. **Schwer vergleichbare Koeffizienten**: <br>\n",
    "In Regressionsmodellen sind die Koeffizienten bei unterschiedlichen Skalen der Variablen nicht direkt vergleichbar.\n",
    "\n",
    "5. **Falsche Regularisierung**: <br>\n",
    "Regularisierungsverfahren wie Lasso oder Ridge behandeln unterschiedlich skalierte Variablen ungleich.\n",
    "\n",
    "Daten sollten skaliert werden, um diese Verzerrungen zu vermeiden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
