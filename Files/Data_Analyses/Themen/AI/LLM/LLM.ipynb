{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f8ecc5-4cda-4899-93f2-3f89981f3724",
   "metadata": {},
   "source": [
    "# Large Language Model (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc398802-dcb4-47ca-a348-dce13f6f183a",
   "metadata": {},
   "source": [
    "**LLM** steht für **Large Language Model** und beschreibt ein großes, vortrainiertes neuronales Netz, das in der Lage ist, natürliche Sprache zu verstehen und zu generieren. Diese Modelle basieren in der Regel auf der **Transformer-Architektur**, die in der Verarbeitung von sequentiellen Daten (wie Text) besonders gut ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c6747-3936-4183-915b-1b9521bd3a6a",
   "metadata": {},
   "source": [
    "### Hauptmerkmale von LLMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50082ad-4ce2-4f6f-8a3c-15122636a46f",
   "metadata": {},
   "source": [
    "1. **Größe und Umfang**: LLMs bestehen aus Milliarden von Parametern und wurden auf riesigen Mengen an Textdaten trainiert. Diese enorme Größe erlaubt es ihnen, komplexe Muster in der Sprache zu lernen und eine hohe Genauigkeit bei Aufgaben wie Textverständnis, Übersetzung und Generierung zu erreichen.\n",
    "\n",
    "2. **Vortrainiert auf großen Datenmengen**: LLMs werden auf großen Textkorpora aus dem Internet, Büchern, Wikipedia, Foren und anderen Quellen trainiert. Dies ermöglicht ihnen, eine breite Palette von Sprachstrukturen, Grammatikregeln und Fakten zu erlernen.\n",
    "\n",
    "3. **Feinjustierung (Fine-Tuning)**: Nach dem Vortraining können LLMs für spezifische Aufgaben oder Domänen weiter trainiert werden. Dies wird als \"Feinjustierung\" bezeichnet, bei der das Modell an spezifische Anforderungen angepasst wird, z. B. in der Medizin, Technik oder Rechtswissenschaft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eb28f-fe56-41c1-b076-6bc2c93bf36e",
   "metadata": {},
   "source": [
    "### Transformer-Architektur:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a4c58-fa0a-4a4c-a498-9d8d23aad666",
   "metadata": {},
   "source": [
    "LLMs basieren häufig auf der **Transformer-Architektur**, die 2017 eingeführt wurde. Diese Architektur nutzt einen **Self-Attention-Mechanismus**, der es dem Modell ermöglicht, relevante Teile einer Sequenz zu identifizieren und zu gewichten, unabhängig davon, wie weit diese voneinander entfernt sind. Das ist besonders nützlich bei der Verarbeitung von längeren Texten, wo frühere Methoden, wie RNNs und LSTMs, oft Schwierigkeiten hatten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c02dc-7313-4dd9-82c0-7211c5e6d199",
   "metadata": {},
   "source": [
    "### Anwendungen von LLMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7bd405-47ad-4c85-b98a-75a8b0601478",
   "metadata": {},
   "source": [
    "- **Textgenerierung**: LLMs können menschenähnlichen Text erzeugen, z. B. für kreative Texte, Chatbots oder Nachrichtenzusammenfassungen.\n",
    "- **Frage-Antwort-Systeme**: Sie können auf natürlich gestellte Fragen antworten, basierend auf ihrem Training auf allgemeinen Texten.\n",
    "- **Übersetzung**: LLMs werden in maschinellen Übersetzungsdiensten eingesetzt.\n",
    "- **Automatische Vervollständigung**: In Texteditoren oder Programmiertools zur automatischen Vervollständigung von Sätzen oder Code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a53abf-542d-4fa5-8fc6-b77f4cec4f5a",
   "metadata": {},
   "source": [
    "### Beispiele für bekannte LLMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28a64e-6a58-4794-978e-baa4a3066583",
   "metadata": {},
   "source": [
    "1. **GPT (Generative Pre-trained Transformer)**: Entwickelt von OpenAI, gehört zu den bekanntesten LLMs. Die verschiedenen Versionen (GPT-2, GPT-3, GPT-4) können beeindruckend komplexe Texte erzeugen.\n",
    "2. **BERT (Bidirectional Encoder Representations from Transformers)**: Entwickelt von Google und verwendet, um Wörter in ihrem Kontext zu verstehen, was es nützlich für Aufgaben wie Fragenbeantwortung und Textklassifikation macht.\n",
    "3. **T5 (Text-to-Text Transfer Transformer)**: Ein Modell von Google, das Aufgaben im NLP als Text-zu-Text-Probleme formuliert, was seine Anwendung sehr flexibel macht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edee043-4163-4ed9-8023-632560642ede",
   "metadata": {},
   "source": [
    "### Beispiel für ein LLM:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650adf4b-317a-48f3-b29d-058529a965aa",
   "metadata": {},
   "source": [
    "Das hier ist ein Beispiel für den Aufruf eines GPT-Modells, das von OpenAI bereitgestellt wird, um Text zu generieren:\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "# API-Schlüssel\n",
    "openai.api_key = 'dein-api-schlüssel'\n",
    "\n",
    "# Anfrage an GPT-3.5 zum Generieren von Text\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=\"Erkläre den Unterschied zwischen LSTM und LLM\",\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Ausgabe des generierten Texts\n",
    "print(response.choices[0].text.strip())\n",
    "```\n",
    "\n",
    "In diesem Beispiel sendet der Code eine Anfrage an ein GPT-basiertes Modell und lässt sich Text generieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a21a1b-47b1-4089-b722-a7b984225079",
   "metadata": {},
   "source": [
    "### Unterschied zu LSTM:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96d372-a633-4ae5-83ca-67e36444e356",
   "metadata": {},
   "source": [
    "Während **LSTM** ein Modelltyp ist, der speziell für die Verarbeitung von sequentiellen Daten entwickelt wurde (z. B. Zeitreihen oder Texte mit chronologischen Abhängigkeiten), bezieht sich **LLM** auf viel größere Modelle, die oft auf der Transformer-Architektur basieren und auf die Verarbeitung von Sprache spezialisiert sind. LSTMs waren vor den Transformer-basierten LLMs populär, sind aber für die Verarbeitung großer Mengen an Daten weniger effizient als Transformer-Modelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea232155-162f-475b-a97d-43ae922fd301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
