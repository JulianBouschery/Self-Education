{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c900df1d-14aa-4c9a-8739-16a8502d291a",
   "metadata": {},
   "source": [
    "# Größe des Datensatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493def09-f567-4c09-be5a-c7053bbec32a",
   "metadata": {},
   "source": [
    "### 1. **Größe eines Datensatzes oder der Grundgesamtheit**\n",
    "Es gibt keine feste Formel, die universell bestimmt, wie groß ein Datensatz oder eine Grundgesamtheit sein muss. Es hängt von verschiedenen Faktoren ab, wie:\n",
    "\n",
    "- **Komplexität des Modells**: Komplexere Modelle (wie tiefe neuronale Netze) benötigen größere Datensätze, um gut zu generalisieren.\n",
    "- **Varianz der Daten**: Wenn die Daten stark variieren, sind größere Stichproben erforderlich, um das gesamte Spektrum der Daten abzudecken.\n",
    "- **Ziel der Analyse**: Wenn das Ziel präzise Schätzungen oder Klassifikationen sind, benötigt man in der Regel mehr Daten.\n",
    "\n",
    "**Faustformel**:\n",
    "Eine oft zitierte Faustformel ist, dass man **10 Mal so viele Datenpunkte wie Modellparameter** haben sollte. Das bedeutet, wenn dein Modell 10 Parameter (Features) hat, wären 100 Datenpunkte eine gute Basis, um das Modell zuverlässig zu trainieren.\n",
    "\n",
    "Für statistische Schätzungen, z.B. in Umfragen, kann die **Stichprobenformel** angewandt werden:\n",
    "\n",
    "\\[\n",
    "n = \\frac{Z^2 \\cdot p \\cdot (1 - p)}{e^2}\n",
    "\\]\n",
    "\n",
    "- **n** = benötigte Stichprobengröße\n",
    "- **Z** = Z-Wert (abhängig vom gewünschten Konfidenzniveau, z.B. 1.96 für 95%)\n",
    "- **p** = erwarteter Anteil in der Grundgesamtheit (z.B. 0.5 bei maximaler Unsicherheit)\n",
    "- **e** = Fehlerspanne (z.B. 0.05 für 5% Fehler)\n",
    "\n",
    "### 2. **Formeln zur Bewertung der Modellqualität**\n",
    "Die Qualität eines Modells kann mit verschiedenen Kennzahlen bewertet werden, abhängig davon, ob es sich um ein **Regression**- oder **Klassifikationsmodell** handelt.\n",
    "\n",
    "#### Für **Regressionsmodelle**:\n",
    "- **R² (Bestimmtheitsmaß)**:\n",
    "  \\[\n",
    "  R^2 = 1 - \\frac{\\sum (y_i - \\hat{y_i})^2}{\\sum (y_i - \\bar{y})^2}\n",
    "  \\]\n",
    "  - Misst, wie gut die unabhängigen Variablen die abhängige Variable erklären. Werte nahe 1 deuten auf ein gutes Modell hin.\n",
    "  \n",
    "- **Mean Squared Error (MSE)**:\n",
    "  \\[\n",
    "  MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "  \\]\n",
    "  - Durchschnitt der quadrierten Fehler. Je niedriger, desto besser das Modell.\n",
    "\n",
    "- **Mean Absolute Error (MAE)**:\n",
    "  \\[\n",
    "  MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|\n",
    "  \\]\n",
    "  - Durchschnitt der absoluten Fehler. MAE ist robuster gegen Ausreißer als MSE.\n",
    "\n",
    "#### Für **Klassifikationsmodelle**:\n",
    "- **Accuracy (Genauigkeit)**:\n",
    "  \\[\n",
    "  Accuracy = \\frac{\\text{Richtig Positive} + \\text{Richtig Negative}}{\\text{Gesamtanzahl der Beispiele}}\n",
    "  \\]\n",
    "  - Anteil der korrekt klassifizierten Beobachtungen. Allerdings kann es bei unausgeglichenen Datensätzen irreführend sein.\n",
    "\n",
    "- **Precision und Recall**:\n",
    "  - **Precision** (Präzision):\n",
    "    \\[\n",
    "    Precision = \\frac{\\text{Richtig Positive}}{\\text{Richtig Positive} + \\text{Falsch Positive}}\n",
    "    \\]\n",
    "    - Anteil der korrekt als positiv klassifizierten Ergebnisse.\n",
    "  \n",
    "  - **Recall** (Sensitivität):\n",
    "    \\[\n",
    "    Recall = \\frac{\\text{Richtig Positive}}{\\text{Richtig Positive} + \\text{Falsch Negative}}\n",
    "    \\]\n",
    "    - Anteil der tatsächlich positiven Fälle, die korrekt erkannt wurden.\n",
    "\n",
    "- **F1-Score**:\n",
    "  \\[\n",
    "  F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\n",
    "  \\]\n",
    "  - Harmonic Mean von Precision und Recall. Gut bei unausgeglichenen Datensätzen.\n",
    "\n",
    "- **ROC AUC (Receiver Operating Characteristic Area Under Curve)**:\n",
    "  - AUC misst die Fähigkeit des Modells, zwischen Klassen zu unterscheiden. Ein Wert von 1 ist ideal, während 0.5 bedeutet, dass das Modell nicht besser als Zufall ist.\n",
    "\n",
    "### Zusammenfassung:\n",
    "- Die Größe eines Datensatzes hängt von der Komplexität und den Zielen ab, und eine Faustregel besagt, dass du etwa 10 Mal so viele Datenpunkte wie Parameter haben solltest.\n",
    "- Die Modellqualität kann durch Kennzahlen wie **R²**, **MSE**, **Accuracy**, **Precision**, **Recall**, und **F1-Score** bewertet werden, je nach Modelltyp (Regression oder Klassifikation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef05146-a736-4720-9b3d-1c9aebb68fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
