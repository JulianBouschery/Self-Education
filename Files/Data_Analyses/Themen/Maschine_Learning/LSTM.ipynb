{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a92a12e-5fa9-46fb-a7e6-1688df55ced3",
   "metadata": {},
   "source": [
    "# LSTM (Long Short Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae2bc4-a9d4-4aff-8ac3-b6d0d2e354aa",
   "metadata": {},
   "source": [
    "Ein **Long Short-Term Memory** (LSTM) Netzwerk ist eine spezielle Art eines rekurrenten neuronalen Netzwerks (RNN), die entwickelt wurde, um Abhängigkeiten in Sequenzen besser zu modellieren. LSTMs sind besonders effektiv, wenn es darum geht, sich über längere Zeiträume an Informationen zu erinnern, und werden häufig in Anwendungen wie Sprachverarbeitung, Zeitreihenanalyse und Übersetzungen verwendet.\n",
    "\n",
    "### Grundlegendes Konzept\n",
    "\n",
    "LSTMs bestehen aus einer Folge von \"Zellen\" oder \"Zuständen\", die Informationen durch die Zeit weitergeben und entscheiden, was sie behalten und was sie verwerfen. Dies wird durch drei zentrale \"Gates\" gesteuert, die zusammenarbeiten, um die Information zu steuern:\n",
    "\n",
    "1. **Forget Gate (Vergessens-Tor)**: Entscheidet, welche Informationen aus dem vorherigen Zustand vergessen werden sollen.\n",
    "2. **Input Gate (Eingangs-Tor)**: Bestimmt, welche neuen Informationen in den Zellzustand eingefügt werden sollen.\n",
    "3. **Output Gate (Ausgangs-Tor)**: Entscheidet, welche Informationen für den nächsten Zeitschritt ausgegeben werden.\n",
    "\n",
    "### Details der LSTM-Mechanik\n",
    "\n",
    "1. **Vergessen und Merken**:\n",
    "   - Zuerst entscheidet das **Forget Gate**, welche Informationen aus dem vorherigen Zellzustand $C_{t-1}$ verworfen werden sollen. Dazu wird der vorherige Zustand und die aktuelle Eingabe durch eine Sigmoid-Aktivierung geleitet, die Werte zwischen 0 (vollständiges Vergessen) und 1 (vollständiges Behalten) liefert.\n",
    "\n",
    "2. **Aktualisieren des Zellzustands**:\n",
    "   - Das **Input Gate** entscheidet, welche neuen Informationen hinzukommen. Es nutzt eine Sigmoid-Aktivierung, um festzulegen, welche Werte geändert werden, und eine tanh-Aktivierung, die die neuen möglichen Werte berechnet.\n",
    "   - Diese Werte werden kombiniert, um den Zellzustand zu aktualisieren: $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$, wobei $f_t$ das Forget Gate und $i_t * \\tilde{C}_t$ die neuen Informationen darstellt.\n",
    "\n",
    "3. **Ausgabe und Weitergabe**:\n",
    "   - Das **Output Gate** verwendet ebenfalls Sigmoid- und tanh-Aktivierungen, um den Zellzustand $C_t$ so zu transformieren, dass nur relevante Informationen nach außen weitergegeben werden.\n",
    "\n",
    "### Beispiel in Python\n",
    "\n",
    "Hier ist eine vereinfachte Implementierung eines LSTM in Python mit NumPy, die die Kernoperationen für ein einzelnes Zeitschritt beschreibt. In der Praxis verwenden Deep-Learning-Frameworks wie PyTorch oder TensorFlow bereits optimierte LSTM-Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6acec6-6eae-4820-99e1-d9acd6b4ec20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuer versteckter Zustand (h_t): [0.390636   0.44603029]\n",
      "Neuer Zellzustand (C_t): [0.5570813  0.67245129]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid und Tanh Funktionen\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Beispiel LSTM Schritt\n",
    "def lstm_step(x_t, h_prev, C_prev, Wf, Wi, Wo, Wc, bf, bi, bo, bc):\n",
    "    # Vergessens-Tor\n",
    "    f_t = sigmoid(np.dot(Wf, np.hstack([h_prev, x_t])) + bf)\n",
    "    # Eingangs-Tor\n",
    "    i_t = sigmoid(np.dot(Wi, np.hstack([h_prev, x_t])) + bi)\n",
    "    # Neues Zellgedächtnis\n",
    "    C_tilda = tanh(np.dot(Wc, np.hstack([h_prev, x_t])) + bc)\n",
    "    # Zellzustand aktualisieren\n",
    "    C_t = f_t * C_prev + i_t * C_tilda\n",
    "    # Ausgangs-Tor\n",
    "    o_t = sigmoid(np.dot(Wo, np.hstack([h_prev, x_t])) + bo)\n",
    "    # Ausgabe (neuer versteckter Zustand)\n",
    "    h_t = o_t * tanh(C_t)\n",
    "    \n",
    "    return h_t, C_t\n",
    "\n",
    "# Beispielhafte Initialisierung von Eingabevektor x_t, vorherigem Zustand h_prev und Zellzustand C_prev\n",
    "x_t = np.array([0.5, 0.1])  # Aktueller Eingabevektor\n",
    "h_prev = np.array([0.1, 0.2])  # Vorheriger versteckter Zustand\n",
    "C_prev = np.array([0.0, 0.0])  # Vorheriger Zellzustand\n",
    "\n",
    "# Beispielhafte Initialisierung von Gewichten und Biases\n",
    "Wf = np.random.rand(2, 4)  # Vergessens-Tor-Gewichte\n",
    "Wi = np.random.rand(2, 4)  # Eingangs-Tor-Gewichte\n",
    "Wo = np.random.rand(2, 4)  # Ausgangs-Tor-Gewichte\n",
    "Wc = np.random.rand(2, 4)  # Zell-Gewichte\n",
    "bf = np.random.rand(2)  # Vergessens-Tor-Bias\n",
    "bi = np.random.rand(2)  # Eingangs-Tor-Bias\n",
    "bo = np.random.rand(2)  # Ausgangs-Tor-Bias\n",
    "bc = np.random.rand(2)  # Zell-Bias\n",
    "\n",
    "# Berechne den nächsten Schritt\n",
    "h_t, C_t = lstm_step(x_t, h_prev, C_prev, Wf, Wi, Wo, Wc, bf, bi, bo, bc)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(\"Neuer versteckter Zustand (h_t):\", h_t)\n",
    "print(\"Neuer Zellzustand (C_t):\", C_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d74258-abf1-44df-82df-e5c3f147e842",
   "metadata": {},
   "source": [
    "### Zusammenfassung\n",
    "- **LSTMs speichern und vergessen Informationen kontrolliert** durch die Kombination von Vergessens-, Eingangs- und Ausgangs-Toren.\n",
    "- **Langfristige und kurzfristige Abhängigkeiten** können durch die Fähigkeit des LSTMs, Zustände über viele Zeitschritte hinweg zu speichern, modelliert werden.\n",
    "  \n",
    "Diese Eigenschaften machen LSTMs ideal für sequenzielle Daten, bei denen es wichtig ist, Kontext über lange Sequenzen hinweg zu bewahren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11382e66-7566-413e-8753-edb002666731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Gib eine Zahl ein (oder 'stop' zum Beenden):  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuer versteckter Zustand (h_t): [0.74110926 0.7248048 ]\n",
      "Neuer Zellzustand (C_t): [0.99996321 0.91868246]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Gib eine Zahl ein (oder 'stop' zum Beenden):  29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuer versteckter Zustand (h_t): [0.96391331 0.95767777]\n",
      "Neuer Zellzustand (C_t): [1.99996321 1.91710103]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Gib eine Zahl ein (oder 'stop' zum Beenden):  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuer versteckter Zustand (h_t): [0.95661454 0.99098427]\n",
      "Neuer Zellzustand (C_t): [2.99668197 2.85288585]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Gib eine Zahl ein (oder 'stop' zum Beenden):  11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuer versteckter Zustand (h_t): [0.9866432  0.99879423]\n",
      "Neuer Zellzustand (C_t): [3.99648543 3.82422194]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Gib eine Zahl ein (oder 'stop' zum Beenden):  18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuer versteckter Zustand (h_t): [0.99808146 0.99986408]\n",
      "Neuer Zellzustand (C_t): [4.99648436 4.81702745]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Gib eine Zahl ein (oder 'stop' zum Beenden):  99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuer versteckter Zustand (h_t): [0.99998762 0.99998228]\n",
      "Neuer Zellzustand (C_t): [5.99648436 5.81702744]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Gib eine Zahl ein (oder 'stop' zum Beenden):  stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation beendet.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid- und Tanh-Funktionen\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# LSTM-Zellschritt-Funktion\n",
    "def lstm_step(x_t, h_prev, C_prev, Wf, Wi, Wo, Wc, bf, bi, bo, bc):\n",
    "    # Vergessens-Tor\n",
    "    f_t = sigmoid(np.dot(Wf, np.hstack([h_prev, x_t])) + bf)\n",
    "    # Eingangs-Tor\n",
    "    i_t = sigmoid(np.dot(Wi, np.hstack([h_prev, x_t])) + bi)\n",
    "    # Neues Zellgedächtnis\n",
    "    C_tilda = tanh(np.dot(Wc, np.hstack([h_prev, x_t])) + bc)\n",
    "    # Zellzustand aktualisieren\n",
    "    C_t = f_t * C_prev + i_t * C_tilda\n",
    "    # Ausgangs-Tor\n",
    "    o_t = sigmoid(np.dot(Wo, np.hstack([h_prev, x_t])) + bo)\n",
    "    # Ausgabe (neuer versteckter Zustand)\n",
    "    h_t = o_t * tanh(C_t)\n",
    "    \n",
    "    return h_t, C_t\n",
    "\n",
    "# Initialisierung des Zustands\n",
    "hidden_size = 2  # Verdeckter Zustand (klein gehalten für einfaches Verständnis)\n",
    "h_prev = np.zeros(hidden_size)  # Anfangs versteckter Zustand\n",
    "C_prev = np.zeros(hidden_size)  # Anfangs Zellzustand\n",
    "\n",
    "# Beispielhafte Initialisierung von Gewichten und Biases\n",
    "Wf = np.random.rand(hidden_size, hidden_size + 1)  # Vergessens-Tor-Gewichte\n",
    "Wi = np.random.rand(hidden_size, hidden_size + 1)  # Eingangs-Tor-Gewichte\n",
    "Wo = np.random.rand(hidden_size, hidden_size + 1)  # Ausgangs-Tor-Gewichte\n",
    "Wc = np.random.rand(hidden_size, hidden_size + 1)  # Zell-Gewichte\n",
    "bf = np.random.rand(hidden_size)  # Vergessens-Tor-Bias\n",
    "bi = np.random.rand(hidden_size)  # Eingangs-Tor-Bias\n",
    "bo = np.random.rand(hidden_size)  # Ausgangs-Tor-Bias\n",
    "bc = np.random.rand(hidden_size)  # Zell-Bias\n",
    "\n",
    "# Schleife zur kontinuierlichen Eingabe und Beobachtung des LSTM-Zustands\n",
    "while True:\n",
    "    # Eingabe des Benutzers\n",
    "    user_input = input(\"Gib eine Zahl ein (oder 'stop' zum Beenden): \")\n",
    "    if user_input.lower() == 'stop':\n",
    "        print(\"Simulation beendet.\")\n",
    "        break\n",
    "    \n",
    "    # Konvertiere Eingabe in einen float (0 wenn Eingabe ungültig ist)\n",
    "    try:\n",
    "        x_t = np.array([float(user_input)])\n",
    "    except ValueError:\n",
    "        print(\"Ungültige Eingabe. Bitte eine Zahl eingeben.\")\n",
    "        continue\n",
    "\n",
    "    # Führe einen LSTM-Schritt aus\n",
    "    h_prev, C_prev = lstm_step(x_t, h_prev, C_prev, Wf, Wi, Wo, Wc, bf, bi, bo, bc)\n",
    "    \n",
    "    # Ausgabe des aktuellen Zustands\n",
    "    print(f\"Neuer versteckter Zustand (h_t): {h_prev}\")\n",
    "    print(f\"Neuer Zellzustand (C_t): {C_prev}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3092b8-95d1-4578-99c7-e65eefc4f038",
   "metadata": {},
   "source": [
    "### Erklärung des Codes:\n",
    "\n",
    "1. **LSTM-Zellschritt**: Wir haben eine Funktion `lstm_step`, die einen Zeitschritt des LSTMs durchführt, indem sie die Gates anwendet und die versteckten Zustände sowie Zellzustände aktualisiert.\n",
    "  \n",
    "2. **User-Eingabe**: Der Code wartet auf eine Zahl, die als Eingabe durch `input()` abgefragt wird. Der Wert wird dann als aktueller Eingabevektor `x_t` für das LSTM verwendet. Wenn der Benutzer \"stop\" eingibt, wird die Schleife beendet.\n",
    "\n",
    "3. **Speicherung und Vergessen**: Nach jeder Eingabe wird der Zustand ausgegeben, sodass du sehen kannst, wie der `C_t`-Zellzustand auf neue Eingaben reagiert und alte Werte mit der Zeit verblassen.\n",
    "\n",
    "### Interpretation der Zustandsänderungen\n",
    "\n",
    "- Beobachte, wie `C_prev` nach mehreren Eingaben entweder eine Kombination aus alten und neuen Informationen enthält oder sich langsam stabilisiert, je nachdem, wie stark der **Forget Gate** den Wert „verblassen“ lässt.\n",
    "- Der `h_prev` gibt den **aktuellen versteckten Zustand** an, der auch von den neuen Eingaben beeinflusst wird und die Ausgabe des LSTMs für diesen Zeitschritt ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f1890-e73e-4bf3-ac5a-dc7f9463546b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
