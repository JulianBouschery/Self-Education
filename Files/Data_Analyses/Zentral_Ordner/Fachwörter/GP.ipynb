{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba3de07-9b4c-4972-981e-6abeaabdee5a",
   "metadata": {},
   "source": [
    "# Vanishing / Exploding Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb536e33-3b71-40ee-aee2-ee49c01e1994",
   "metadata": {},
   "source": [
    "Das **Vanishing Gradient Problem** tritt bei tiefen neuronalen Netzen, insbesondere in **[Recurrent Neural Networks (RNNs)](RNN.ipynb)** und **[Deep Feedforward Networks](DFN.ipynb)**, auf. Dabei werden die **Gradienten** (Ableitungen), die für das Anpassen der Gewichte während des Trainings verwendet werden, in den frühen Schichten sehr klein. Dadurch lernen die vorderen Schichten kaum, da die Gewichte nur minimal aktualisiert werden. Dies verhindert das effektive Training tiefer Netze.\n",
    "\n",
    "Das **Exploding Gradient Problem** ist das Gegenteil: Hier wachsen die Gradienten bei der Rückpropagierung exponentiell an. Dies führt dazu, dass die Gewichte sehr große Werte annehmen und das Training instabil oder sogar unbrauchbar wird, da das Modell keine sinnvollen Parameter mehr lernt.\n",
    "\n",
    "Lösungen umfassen **[LSTM](LSTM.ipynb)**, **[Batch Normalization](Batch_Normalization.ipynb)** oder [**ResNet**-Architekturen](ResNet.ipynb), die das **Vanishing Gradient Problem** abmildern und Techniken wie **Gradient Clipping**, um das **Exploding Gradient Problem** zu kontrollieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa974b-4642-4261-8f02-e53bedce6dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
