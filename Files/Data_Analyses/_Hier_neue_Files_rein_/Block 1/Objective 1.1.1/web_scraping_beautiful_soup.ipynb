{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5519507-fa3e-4c74-bd2f-44b02c70d2ec",
   "metadata": {},
   "source": [
    "# Was ist Beautiful Soup?\n",
    "\n",
    "BeautifulSoup ist ein Python-Paket, das entwickelt wurde, um das Parsen (Analysieren) und Navigieren von HTML- und XML-Dokumenten zu erleichtern. Sie ermöglicht es, Webdaten einfach zu extrahieren und zu verarbeiten. BeautifulSoup arbeitet gut in Kombination mit der `requests`-Bibliothek, um Webseiten abzurufen und zu analysieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fddd15-a1fb-4c2d-a82f-a802830dc48e",
   "metadata": {},
   "source": [
    "---\n",
    "# Wie arbeitet BeautifulSoup?\n",
    "\n",
    "BeautifulSoup ermöglicht es, HTML- oder XML-Dokumente in ein Baumdiagramm zu konvertieren, das leicht navigierbar und manipulierbar ist. Es extrahiert Daten, indem es Tags, Attribute oder den Textinhalt durchsucht und verarbeitet.\n",
    "\n",
    "Ein typisches Web-Scraping-Szenario besteht aus den folgenden Schritten:\n",
    "\n",
    "- HTTP-Anfrage senden: Die Webseite wird geladen, um den HTML-Quellcode zu erhalten.\n",
    "- HTML-Dokument parsen: Das HTML-Dokument wird mit BeautifulSoup in einen Baum umgewandelt.\n",
    "- Daten extrahieren: Bestimmte Tags, Klassen oder IDs werden durchsucht, um die benötigten Daten zu extrahieren.\n",
    "\n",
    "https://datascientest.com/de/beautiful-soup-einfuehrung-in-web-scraping-mit-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f37bd80-90e7-46de-aa5d-e6ff33c014c8",
   "metadata": {},
   "source": [
    "# Vorteile\n",
    "\n",
    "- Einfach zu verwenden: Die API ist einfach zu erlernen und zu verstehen.\n",
    "- Flexibilität: BeautifulSoup kann mit verschiedenen Parsern wie html.parser, lxml oder html5lib verwendet werden.\n",
    "- Leistungsfähig: Es ermöglicht den Zugriff auf HTML-Elemente und Attribute und bietet verschiedene Methoden, um den HTML-Baum zu navigieren und zu durchsuchen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924506d0-df72-498c-a44a-6d150fcc276c",
   "metadata": {},
   "source": [
    "---\n",
    "# Beautiful Soup verwenden\n",
    "\n",
    "**zunächst installieren:**\n",
    "\n",
    "```bash\n",
    "pip install beautifulsoup4 requests\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Entwicklertools öffnen**, z.B. auf Firefox:\n",
    "\n",
    "Rechtsklicke irgendwo auf der Webseite und wähle im Kontextmenü „Seitenquelltext anzeigen“ aus. Dadurch wird der HTML-Code der Seite in einem neuen Tab geöffnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea6de4-5405-4f92-aae0-119bde8c4c7e",
   "metadata": {},
   "source": [
    "## Erste Schritte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b13b65-19d4-4c96-bc79-d6efd2b59ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Example Domain\n",
      "  </title>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <m\n",
      "Der Titel der Webseite lautet: Example Domain\n"
     ]
    }
   ],
   "source": [
    "# Bibliotheken importieren\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Eine einfache Webseite abrufen\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "\n",
    "# HTML der Webseite parsen\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Die ersten 100 Zeichen des HTML anzeigen\n",
    "print(soup.prettify()[:100])\n",
    "\n",
    "# Titel der Webseite extrahieren\n",
    "title = soup.title.string\n",
    "print(f\"Der Titel der Webseite lautet: {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff54c2d-4e3d-4eb1-a73c-10654a587f1d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "**Erklärung der Befehle:**\n",
    "\n",
    "- requests.get(url): &nbsp;&nbsp; Diese Funktion sendet eine HTTP-Anfrage an die angegebene URL und gibt die Antwort zurück.\n",
    "- BeautifulSoup(response.text, 'html.parser'): &nbsp;&nbsp; Diese Funktion analysiert das HTML-Dokument und erstellt eine BeautifulSoup-Objektstruktur.\n",
    "- soup.prettify(): &nbsp;&nbsp;Gibt den geparsten HTML-Code in einer gut formatierten Weise zurück.\n",
    "- soup.title.string: &nbsp;&nbsp;Extrahiert den Titel der Webseite aus dem title-Tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2be81-7c9b-4151-9068-8a8746ce85ca",
   "metadata": {},
   "source": [
    "## im DOM-Baum navigieren und HTML-Elemente extrahieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d164c3d8-00fd-46da-a3ca-66da42f41462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "# Beispiel: Alle Links auf der Webseite extrahieren\n",
    "links = soup.find_all('a')  # Alle <a>-Tags finden\n",
    "for link in links:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f240e86-6c7c-42da-9d2a-d10f6587a48a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Erklärung der Befehle:**\n",
    "\n",
    "- soup.find_all('a'): Findet alle Tags, die den Tag-Namen `<a>` haben, was typischerweise Links auf einer Webseite sind.\n",
    "- link.get('href'): Extrahiert den href-Attributwert (die URL) eines Links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458362bc-e165-464c-96a8-399b59fb2429",
   "metadata": {},
   "source": [
    "## Komplexe Suchfunktionen und Attribute filtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24392ac3-8ad1-43f2-b5ed-210c878620eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.\n"
     ]
    }
   ],
   "source": [
    "# Beispiel: Ersten Absatz der Webseite extrahieren\n",
    "first_paragraph = soup.find('p').text  # Ersten <p>-Tag finden und Text extrahieren\n",
    "print(first_paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90094f-a358-4414-9319-484a312bba4f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Erklärung der Befehle:**\n",
    "\n",
    "- soup.find('p'): Findet den ersten $<p>-Tag$ (Absatz).\n",
    "- .text: Extrahiert den Textinhalt des gefundenen Tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c996a-4daf-428c-abe6-0e282dafff92",
   "metadata": {},
   "source": [
    "Suche nach dem Wort \"domain\" in dem Text first_paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a113d8d-7bd8-43f6-be98-760afb4ad0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Wort \"domain\" kommt 2 Mal im Absatz vor.\n",
      "Das Wort \"domain\" wurde an der Position 5 gefunden.\n",
      "Das Wort \"domain\" wurde an der Position 83 gefunden.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Suche nach dem Wort \"domain\"\n",
    "word_to_search = \"domain\"\n",
    "\n",
    "# Anzahl der Vorkommen des Wortes \"domain\"\n",
    "occurrences = first_paragraph.lower().count(word_to_search)\n",
    "\n",
    "print(f'Das Wort \"{word_to_search}\" kommt {occurrences} Mal im Absatz vor.')\n",
    "\n",
    "# Position(en) des Wortes \"domain\" im Text finden\n",
    "start_pos = 0\n",
    "while True:\n",
    "    start_pos = first_paragraph.lower().find(word_to_search, start_pos)\n",
    "    if start_pos == -1:\n",
    "        break\n",
    "    print(f'Das Wort \"{word_to_search}\" wurde an der Position {start_pos} gefunden.')\n",
    "    start_pos += len(word_to_search)  # Startet die Suche nach dem nächsten Vorkommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bedb90-af3e-448b-b353-77282b2814df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131ef9b6-b374-4ada-94d0-271652c72179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Alle Links mit dem Attribut class=\"external\" extrahieren\n",
    "external_links = soup.find_all('a', class_='external')\n",
    "for link in external_links:\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc18c7-b0a9-4b2b-8d2e-38c20261365b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Erklärung der Befehle:**\n",
    "\n",
    "- soup.find_all('a', class_='external'): Sucht alle <a>-Tags, die das Attribut class=\"external\" haben.\n",
    "- class_='external': In BeautifulSoup wird class_ mit einem Unterstrich verwendet, weil class in Python ein reserviertes Schlüsselwort ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a15ca-0168-4d7d-a459-e2f2fdf07523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc4c0488-4f6e-461a-827b-5142c80a5877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type Attribut des <style>-Tags: text/css\n",
      "\n",
      "CSS Inhalt im <style>-Tag:\n",
      "\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Das <style>-Tag extrahieren\n",
    "style_tag = soup.find('style')\n",
    "\n",
    "# Das type-Attribut extrahieren\n",
    "style_type = style_tag.get('type')\n",
    "print(f\"Type Attribut des <style>-Tags: {style_type}\")\n",
    "\n",
    "# Den CSS-Inhalt extrahieren\n",
    "style_content = style_tag.string\n",
    "print(f\"\\nCSS Inhalt im <style>-Tag:\\n{style_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13bb5f4-969d-4e6a-add0-39514ec39d86",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Erklärung der Befehle:**\n",
    "- find() sucht nach dem $<style>-Tag$.\n",
    "- get('type') extrahiert das type-Attribut des $<style>-Tags.$\n",
    "- string gibt den CSS-Code als String aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a405203-1f0b-4849-a0f3-891fbad376ef",
   "metadata": {},
   "source": [
    "# Beispiele\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf552dbc-ce56-43dc-9486-13c799f28689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "# Beispiel 1:  Web-Seite analysieren\n",
    "\n",
    "# Import der benötigten Bibliotheken\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Schritt 1: HTTP-Anfrage an die Webseite senden\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Schritt 2: HTML-Dokument mit BeautifulSoup parsen\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "# Schritt 3: Daten extrahieren - Beispiel: Alle Links finden\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# Ausgabe der Links\n",
    "for link in links:\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b035e03-df68-4189-9117-d8142ecc90aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['Max', '30']\n",
      "['Anna', '25']\n"
     ]
    }
   ],
   "source": [
    "# Beispiel 2: Extraktion von Tabellem:\n",
    "\n",
    "# HTML-Beispiel mit einer Tabelle\n",
    "html_doc = \"\"\"\n",
    "<table>\n",
    "    <tr><th>Name</th><th>Alter</th></tr>\n",
    "    <tr><td>Max</td><td>30</td></tr>\n",
    "    <tr><td>Anna</td><td>25</td></tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "# HTML parsen\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "\n",
    "# Tabelle finden\n",
    "table = soup.find('table')\n",
    "\n",
    "# Alle Zeilen extrahieren\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Daten aus den Zeilen extrahieren\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text for ele in cols]\n",
    "    print(cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd85f88-a074-44e5-b59b-3c5963db9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel 3: Links extrahieren\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Schritt 1: Webseite abrufen (z.B. Wikipedia Startseite)\n",
    "url = 'https://en.wikipedia.org/wiki/Web_scraping'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Schritt 2: HTML der Webseite parsen\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Den Titel der Seite extrahieren\n",
    "title = soup.title.string\n",
    "print(f\"Seitentitel: {title}\")\n",
    "\n",
    "# Alle Links auf der Seite extrahieren\n",
    "links = soup.find_all('a')\n",
    "print(\"\\nAlle Links auf der Seite:\")\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    if href:  # nur Links mit einer URL\n",
    "        print(href)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d87ca-6f9a-424c-a835-7419910b9921",
   "metadata": {},
   "source": [
    "Erklärung:\n",
    "\n",
    "- soup.title.string extrahiert den Titel der Webseite.\n",
    "- soup.find_all('a') sucht nach allen Links (`<a>`-Tags) auf der Seite und gibt die href-Attribute (URLs) aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abf5f2-67de-447d-95fd-e349784780b8",
   "metadata": {},
   "source": [
    "---\n",
    "# Diskussionen\n",
    "## weitere Python-Bibliotheken, die nützlich für Web Scraping sind\n",
    "\n",
    "1. **Selenium**:\n",
    "   - Selenium ist ein Tool, das verwendet wird, um Webbrowser zu automatisieren. Es eignet sich besonders für dynamische Webseiten, die Inhalte durch JavaScript laden.\n",
    "   - Es kann mit verschiedenen Browsern wie Chrome oder Firefox verwendet werden.\n",
    "   - Selenium ermöglicht Interaktionen mit Webseiten, wie das Ausfüllen von Formularen, Klicken auf Buttons oder das Scrollen der Seite.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Scrapy**:\n",
    "   - Scrapy ist ein leistungsstarkes Web-Scraping-Framework, das für große Scraping-Projekte gut geeignet ist.\n",
    "   - Es unterstützt automatisierte Anfragen an Webseiten, folgt Links und kann Daten durch Pipelines weiterverarbeiten.\n",
    "   - Scrapy ist besonders effizient, da es asynchrone Anfragen unterstützt, was das Scraping von großen Mengen an Daten beschleunigt.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Requests-HTML**:\n",
    "   - Requests-HTML ist eine benutzerfreundliche Bibliothek, die HTML-Dokumente einfach parsen und Inhalte extrahieren kann.\n",
    "   - Sie ermöglicht das Scraping von JavaScript-basierten Inhalten durch eine einfache API.\n",
    "   - Man kann DOM-Elemente ähnlich wie bei Beautiful Soup selektieren und durchsuchen.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **LXML**:\n",
    "   - LXML ist eine schnelle Bibliothek zum Parsen und Manipulieren von XML und HTML.\n",
    "   - Sie kombiniert die Geschwindigkeit des libxml2-Parsers mit der Einfachheit von Beautiful Soup und bietet XPath-Unterstützung für fortgeschrittene Queries.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. **Pyppeteer**:\n",
    "   - Pyppeteer ist eine Python-Portierung von Puppeteer, einer Node.js-Bibliothek zur Steuerung von Chrome oder Chromium über das DevTools-Protokoll.\n",
    "   - Es ist besonders nützlich für das Scraping von Webseiten, die stark auf JavaScript setzen.\n",
    "   - Pyppeteer kann dynamische Inhalte verarbeiten und bietet volle Browser-Funktionalität.\n",
    "\n",
    "<br>\n",
    "\n",
    "6. **MechanicalSoup**:\n",
    "   - MechanicalSoup ist eine leichte Bibliothek, die auf Beautiful Soup und Requests aufbaut und es ermöglicht, Webseiten zu durchsuchen und Formulare auszufüllen.\n",
    "   - Sie eignet sich gut für einfache Interaktionen mit Webseiten und unterstützt Session-Handling.\n",
    "\n",
    "<br>\n",
    "Diese Bibliotheken decken ein breites Spektrum an Web-Scraping-Szenarien ab, von einfachen statischen Seiten bis hin zu komplexen, JavaScript-gesteuerten Webseiten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17d385-3c88-405a-a2f3-05237647693e",
   "metadata": {},
   "source": [
    "---\n",
    "## Welche rechtlichen und ethischen Überlegungen gibt es beim Scraping von Webseiten?\n",
    "\n",
    "### Rechtliche Überlegungen\n",
    "\n",
    "1. **Nutzungsbedingungen der Website (Terms of Service)**:\n",
    "   - Viele Webseiten haben in ihren Nutzungsbedingungen klare Richtlinien zum Scraping. Es kann ausdrücklich verboten sein, Inhalte ohne Erlaubnis automatisiert zu extrahieren.\n",
    "   - Verstöße gegen die Nutzungsbedingungen können zivilrechtliche Konsequenzen nach sich ziehen. Auch wenn das Ignorieren von Nutzungsbedingungen oft nicht strafrechtlich verfolgt wird, kann es dennoch rechtliche Schritte wie Abmahnungen oder Klagen zur Folge haben.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Urheberrecht**:\n",
    "   - Inhalte auf Webseiten, wie Texte, Bilder oder Videos, sind oft urheberrechtlich geschützt. Das Scraping und Wiederveröffentlichen von urheberrechtlich geschützten Inhalten ohne Erlaubnis kann eine Verletzung darstellen.\n",
    "   - Es ist wichtig sicherzustellen, dass die gescrapten Daten entweder gemeinfrei (public domain) sind oder mit der ausdrücklichen Zustimmung des Rechteinhabers verwendet werden.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Datenschutz (GDPR, CCPA)**:\n",
    "   - Scraping von personenbezogenen Daten unterliegt strengen Datenschutzgesetzen wie der Datenschutz-Grundverordnung (GDPR) in der EU oder dem California Consumer Privacy Act (CCPA) in den USA.\n",
    "   - Das Sammeln von personenbezogenen Informationen ohne die Zustimmung der betroffenen Personen kann gegen diese Gesetze verstoßen und empfindliche Geldstrafen nach sich ziehen.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **Robots.txt**:\n",
    "   - Viele Webseiten enthalten eine `robots.txt`-Datei, die angibt, welche Bereiche der Seite von Crawlern indexiert werden dürfen. Auch wenn `robots.txt` technisch nicht durchsetzbar ist, signalisiert es den Wunsch des Website-Betreibers, dass bestimmte Inhalte nicht gecrawlt oder gescraped werden sollten.\n",
    "   - Das Ignorieren von `robots.txt` könnte als missbräuchliches Verhalten ausgelegt werden und rechtliche Folgen haben.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. **Computer Fraud and Abuse Act (CFAA)**:\n",
    "   - In den USA verbietet das CFAA das unautorisierte Zugreifen auf Computersysteme. Dies kann beim Scraping problematisch werden, wenn Webseiten so gestaltet sind, dass das Scraping als „unauthorisierter Zugriff“ interpretiert werden kann.\n",
    "   - Ähnliche Gesetze existieren in anderen Ländern, die den unbefugten Zugang zu Computernetzwerken und Daten regulieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d05ea0-60ad-45ff-adef-2846f110c92f",
   "metadata": {},
   "source": [
    "---\n",
    "### Ethische Überlegungen\n",
    "\n",
    "1. **Serverbelastung und Ressourcenverbrauch**:\n",
    "   - Web Scraping kann, wenn es in großem Umfang und ohne Rücksicht auf die Zielserver durchgeführt wird, den Server stark belasten und die Leistung der Website beeinträchtigen.\n",
    "   - Eine ethische Vorgehensweise ist, Anfragen langsam und kontrolliert zu senden, z. B. durch „Rate Limiting“, um die Belastung der Website zu minimieren.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Einwilligung und Transparenz**:\n",
    "   - Auch wenn das Scraping technisch möglich ist, kann es ethisch problematisch sein, wenn die Website-Betreiber dem nicht zugestimmt haben. Eine vorherige Anfrage oder Erlaubnis zeigt Respekt gegenüber den Inhalten und den Betreibern.\n",
    "   - Transparenz darüber, wie und warum die Daten gesammelt werden, ist ebenfalls ein ethischer Faktor. Besonders bei öffentlich zugänglichen Daten sollte klar kommuniziert werden, wofür die gescrapten Daten genutzt werden.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Verwendung der Daten**:\n",
    "   - Die Nutzung gescrapter Daten für schädliche Zwecke, wie Spam, Betrug, oder um sensible Informationen offenzulegen, ist sowohl rechtlich als auch ethisch fragwürdig.\n",
    "   - Eine verantwortungsvolle Nutzung der Daten bedeutet, dass sie in einer Weise verwendet werden, die weder den Eigentümern der Daten noch den betroffenen Personen Schaden zufügt.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **Privatsphäre der Nutzer**:\n",
    "   - Besonders wenn personenbezogene Daten gescraped werden, sollte die Privatsphäre der betroffenen Nutzer respektiert werden. Dies umfasst den verantwortungsvollen Umgang mit Daten und die Minimierung von potenziellen Risiken durch Datenlecks oder Missbrauch.\n",
    "\n",
    "\n",
    "\n",
    "Während Web Scraping ein mächtiges Werkzeug zur Datenerfassung ist, muss es verantwortungsvoll genutzt werden. Die Beachtung von rechtlichen Rahmenbedingungen, den Interessen der Webseitenbetreiber und der ethischen Implikationen ist entscheidend, um rechtliche Auseinandersetzungen und ethische Verstöße zu vermeiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad446ea1-480a-481c-a691-3eb01bdc0b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
