{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7887cd51-5522-4851-8569-4206d79a09b8",
   "metadata": {},
   "source": [
    "Der Umgang mit fehlenden, falschen oder irreführenden Informationen in Datensätzen ist ein entscheidender Schritt in der Datenbereinigung und -standardisierung. Um die Qualität und Aussagekraft von Analysen sicherzustellen, müssen diese Probleme methodisch und sorgfältig angegangen werden.\n",
    "\n",
    "https://www.inwt-statistics.de/blog/fehlende-werte-verstehen-und-handhaben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6d177-af25-457a-a37e-fcdf2bec69f5",
   "metadata": {},
   "source": [
    "# Syllabus\n",
    "\n",
    "- Address missing, inaccurate, or misleading information.\n",
    "- Tackle specific data quality issues: numerical data problems, duplicate records, invalid data entries, and missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d3882-90eb-4186-a3a1-4e302d08ebd8",
   "metadata": {},
   "source": [
    "---\n",
    "# Umgang mit fehlenden Informationen\n",
    "\n",
    "Fehlende Daten treten in fast allen realen Datensätzen auf, sei es durch Erfassungsfehler, unvollständige Antworten oder technische Probleme.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Identifikation fehlender Werte\n",
    "In Python kann man mit Pandas `isnull()` oder `isna()` verwenden, um fehlende Werte zu identifizieren.\n",
    "<br>\n",
    "\n",
    "```python\n",
    "missing_values = df.isnull().sum()\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "## Entfernen fehlerhafter Daten\n",
    "Wenn der Anteil fehlender Daten gering ist (z.B. <5%) oder die fehlenden Werte zufällig verteilt sind und keinen großen Einfluss auf das Gesamtergebnis haben.\n",
    "\n",
    "- **Zeilen löschen** (`dropna()` in Pandas):<br>\n",
    "  Entfernt Datensätze, in denen eine oder mehrere Werte fehlen.\n",
    "- **Spalten löschen**:<br>\n",
    "  Wenn eine Spalte überwiegend fehlende Werte enthält, kann es sinnvoll sein, die gesamte Spalte zu entfernen.\n",
    "\n",
    "- **Vorteile**:<br>\n",
    "  Einfach anzuwenden und verhindert, dass unvollständige Daten die Analyse verfälschen.\n",
    "- **Nachteile**:<br>\n",
    "  Kann wertvolle Informationen verlieren, insbesondere wenn viele Datenpunkte gelöscht werden.\n",
    "<br>\n",
    "\n",
    "\n",
    "   ```python\n",
    "   df_cleaned = df.dropna()  # Entfernt Zeilen mit fehlenden Werten\n",
    "   ```\n",
    "\n",
    "<br>\n",
    "\n",
    "## Auffüllen fehlender Werte\n",
    "\n",
    "Wenn fehlende Daten systematisch oder nicht zufällig verteilt sind und der Verlust von Daten vermieden werden soll.\n",
    "\n",
    " - **Mittelwert, Median, Modus:** <br>\n",
    "   - **numerische Daten**:<br>\n",
    "     Fehlende Werte können durch den Mittelwert oder Median der Spalte ersetzt werden.\n",
    "   - **kategorische Daten**:<br>\n",
    "     Der Modus (häufigster Wert) kann verwendet werden.\n",
    "   \n",
    "     <br>\n",
    "     \n",
    " - **Vorheriger/Nachfolgender Wert:** <br>\n",
    "    Bei numerischen Daten oder Zeitreihen kann lineare Interpolation verwendet werden, um Zwischenwerte zu schätzen.<br>\n",
    "    Daten werden mit dem letzten vorhandenen (**Backward Fill**, `bfill()`) oder nächsten (**Forward Fill**, `ffill()`) gültigen Wert gefüllt (nützlich für zeitlich geordnete Daten).\n",
    " - **Modellbasierte Imputation:** <br>\n",
    "    Hierbei werden maschinelle Lernmodelle verwendet, um fehlende Werte basierend auf anderen Variablen vorherzusagen.<br>\n",
    "    z.B. K-Nearest Neighbors oder Regression\n",
    "\n",
    "- **Vorteile**:<br>\n",
    "  Erhält die Datenstruktur und erlaubt eine vollständige Analyse.\n",
    "- **Nachteile**:<br>\n",
    "  Kann zu Verzerrungen führen, insbesondere wenn die Schätzungen ungenau sind oder die Daten nicht zufällig fehlen.\n",
    "<br>\n",
    "\n",
    "\n",
    "   ```python\n",
    "   df['column_name'] = df['column_name'].fillna(df['column_name'].mean())  # Auffüllen mit Mittelwert\n",
    "   ```\n",
    "\n",
    "<br>\n",
    "\n",
    "## Spezifische Markierung\n",
    "Fehlende Werte können durch einen speziellen Wert (z.B. `-999`, `None` oder eine Kategorie „Nicht verfügbar“) ersetzt werden, um die fehlenden Daten explizit kenntlich zu machen, ohne sie zu löschen oder zu interpolieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2cd26-3624-4526-97ed-2b1267c0e6a5",
   "metadata": {},
   "source": [
    "---\n",
    "# Wann entfernen, wann auffüllen?\n",
    "\n",
    "Der Umgang mit fehlenden Werten hängt stark vom Kontext der Analyse, der Art der Daten und dem Ziel der Analyse ab. Hier sind Richtlinien, wann welche Strategie sinnvoll ist.\n",
    "\n",
    "- **Fehlende Werte entfernen** ist sinnvoll, wenn nur wenige Werte fehlen, die Werte nicht sinnvoll ersetzt werden können oder wenn es um besonders genaue Analysen geht.\n",
    "- **Fehlende Werte auffüllen** ist angebracht, wenn viele Daten fehlen, wichtige Variablen betroffen sind oder die fehlenden Werte durch andere Variablen oder Modelle vorhergesagt werden können. \n",
    "\n",
    "## Fehlende Werte entfernen\n",
    "\n",
    "### a) Nur wenige fehlende Werte vorhanden\n",
    "- Wenn nur ein kleiner Prozentsatz (z.B. < 5%) der Daten fehlt, kann es oft einfacher und sicherer sein, diese Einträge zu entfernen, anstatt komplizierte Imputationstechniken zu verwenden.\n",
    "- Beispiel:<br>\n",
    "  Wenn 3 von 1000 Datenpunkten fehlen, könnte das Entfernen dieser Werte die Gesamtanalyse nicht signifikant beeinflussen.\n",
    "\n",
    "### b) Spalten/Zeilen mit vielen fehlenden Werten\n",
    "- Wenn eine bestimmte Spalte oder Zeile in einem Datensatz einen großen Anteil fehlender Werte aufweist (z.B. > 50%), kann es sinnvoll sein, die gesamte Spalte oder Zeile zu entfernen, da das Auffüllen nicht mehr verlässlich oder sinnvoll ist.\n",
    "- Beispiel: <br>\n",
    "  Eine Umfrage enthält eine Frage, die von mehr als der Hälfte der Teilnehmer nicht beantwortet wurde. In diesem Fall kann die gesamte Frage irrelevant für die Analyse sein.\n",
    "\n",
    "### c) Werte sind nicht sinnvoll zu ersetzen\n",
    "- Wenn fehlende Werte nicht logisch oder sinnvoll durch andere Werte ersetzt werden können (z.B. wenn ein Datum oder ein einzigartiger Identifikator fehlt), sollte das Entfernen bevorzugt werden. Ein „Ersatzwert“ könnte in diesen Fällen die Daten nur verfälschen.\n",
    "- Beispiel:<br>\n",
    "  Ein eindeutiger Identifikator wie `customer_id` kann nicht einfach interpoliert oder mit einem Standardwert gefüllt werden. Wenn dieser fehlt, macht es mehr Sinn, den gesamten Eintrag zu löschen.\n",
    "\n",
    "### d) Wenn es auf die Integrität der Analyse ankommt\n",
    "- Wenn die Analyse extrem präzise sein muss und bereits geringfügige Veränderungen durch das Auffüllen zu verzerrten Ergebnissen führen könnten, ist das Entfernen vorzuziehen.\n",
    "- Beispiel:<br>\n",
    "  In medizinischen Studien könnten falsche Annahmen über fehlende Werte zu falschen Schlüssen führen.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Fehlende Werte auffüllen (Imputation)\n",
    "\n",
    "#### a) hohe Anzahl an fehlenden Werten vorhanden\n",
    "- Wenn viele Daten fehlen, kann das Entfernen dieser Daten die Analyse erheblich beeinträchtigen, da wertvolle Informationen verloren gehen. In solchen Fällen ist es oft besser, die fehlenden Werte mit geeigneten Techniken aufzufüllen, um die Vollständigkeit der Daten zu bewahren.\n",
    "- Beispiel: In einem medizinischen Datensatz fehlen 20% der Werte in einer bestimmten Spalte. Diese zu entfernen würde einen signifikanten Informationsverlust bedeuten.\n",
    "\n",
    "### b) Daten sind für die Analyse entscheidend\n",
    "- Wenn die fehlenden Werte in wichtigen Variablen liegen, die für die Analyse entscheidend sind, kann das Auffüllen dazu beitragen, die Analyse aufrechtzuerhalten, ohne dass ganze Zeilen oder Spalten entfernt werden.\n",
    "- Beispiel:<br>\n",
    "  Fehlende Verkaufsdaten könnten mit dem Mittelwert oder einem Trendmodell gefüllt werden, um die Vorhersage von Umsätzen nicht zu gefährden.\n",
    "\n",
    "### c) Daten sind lückenhaft, aber systematisch\n",
    "- Wenn die fehlenden Daten einem erkennbaren Muster folgen oder die Möglichkeit besteht, diese aus anderen Variablen zu rekonstruieren, kann das Auffüllen sinnvoll sein.\n",
    "- Beispiel:<br>\n",
    "  In einem Zeitreihendatensatz fehlen Werte nur an bestimmten Wochenenden. Hier könnte eine Interpolation die fehlenden Werte verlässlich rekonstruieren.\n",
    "\n",
    "### d) es gibt Korrelationen mit anderen Variablen\n",
    "- Wenn fehlende Werte durch Korrelationen oder Zusammenhänge mit anderen Variablen vorhergesagt werden können, ist es oft sinnvoll, diese zu nutzen, um die fehlenden Werte zu schätzen. Maschinelle Lernmethoden oder Regressionsmodelle können hier helfen.\n",
    "- Beispiel:<br>\n",
    "  Das Einkommen eines Kunden könnte anhand anderer Faktoren wie Alter, Beruf und Wohnort geschätzt werden.\n",
    "\n",
    "### e) Analyse erfordert robuste Schätzungen\n",
    "- Bei maschinellen Lernverfahren kann das Auffüllen fehlender Werte die Vorhersagefähigkeiten eines Modells verbessern, da so mehr Datenpunkte für das Training erhalten bleiben. Verschiedene Imputationstechniken wie K-Nearest Neighbors (KNN) oder multivariate Imputation können hier hilfreich sein.\n",
    "\n",
    "\n",
    "\n",
    "### Zusammenfassung:\n",
    "\n",
    "\n",
    "Die Wahl der Methode hängt von der Art der Daten und dem Ziel der Analyse ab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ca9f9-7873-44bc-9f22-cea52a980723",
   "metadata": {},
   "source": [
    "---\n",
    "# Umgang mit falschen bzw. ungültigen Informationen\n",
    "\n",
    "Falsche Informationen können durch Tippfehler, falsche Dateneingaben oder technische Störungen entstehen.\n",
    "\n",
    "## Tippfehler\n",
    "\n",
    "- **Textbereinigung**: <br>\n",
    "  Textdaten können normalisiert werden, indem Groß- und Kleinschreibung angeglichen oder überflüssige Leerzeichen entfernt werden (`str.lower()`, `str.strip()`)\n",
    "     \n",
    "- **Vorteile**:<br>\n",
    "  stellt sicher, dass die Daten konsistent und korrekt sind\n",
    "\n",
    "- **Nachteile**: <br>\n",
    "  kann ohne automatisierte Prozesse mühsam sein\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## Ersetzen offensichtlicher Fehler durch logische Werte\n",
    "Identifikation falscher Daten durch spezifische Regeln oder Schwellwerte für die Dateneingaben (z.B. dass Alter > 150 wahrscheinlich falsch ist).\n",
    "\n",
    "- **Schwellenwert**: <br>\n",
    "  Bestimmen eines realistischen Wertebereichs und Ersetzen von Werten außerhalb dieses Bereichs durch Median, Mittelwert oder festgelegten Ersatzwert\n",
    "- **Regeln anwenden**: <br>\n",
    "  Wenn aus Geschäftslogik oder Expertenwissen klare Regeln hervorgehen, was plausible Werte sind, können diese verwendet werden\n",
    "- **Vorteile**: <br>\n",
    "  verbessert die Plausibilität der Daten\n",
    "- **Nachteile**: <br>\n",
    "  erfordert fundierte Kenntnisse über den Datensatz oder das Anwendungsgebiet, um die richtigen Ersatzwerte zu wählen\n",
    "<br>\n",
    "\n",
    "\n",
    "```python\n",
    "df = df[df['age'] <= 150]  # Filterung nach plausiblen Werten\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "## Duplikate\n",
    "\n",
    "Mit Pandas `drop_duplicates()` können doppelte Einträge entfernt werden.\n",
    "\n",
    "```python\n",
    "df_cleaned = df.drop_duplicates()\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "## Outlier\n",
    "\n",
    "Statistische Techniken wie Z-Score oder IQR (Interquartilsabstand) können verwendet werden, um extreme Ausreißer zu identifizieren, die möglicherweise falsch sind.\n",
    "\n",
    "- **Z-Score-Methode**: <br>\n",
    "  Werte, die mehr als eine bestimmte Anzahl von Standardabweichungen vom Mittelwert abweichen, werden als Ausreißer identifiziert\n",
    "- **Interquartilsabstand (IQR)**:<br>\n",
    "  Werte außerhalb des 1,5-fachen des IQR (zwischen dem ersten und dritten Quartil) können als Ausreißer behandelt werden\n",
    "- **Vorteile**:<br>\n",
    "  verbessert die Genauigkeit der Analyse, durch Eliminierung extremer Verzerrungen\n",
    "- **Nachteile**: <br>\n",
    "  Risiko, legitime extreme Werte fälschlicherweise als Ausreißer zu behandeln\n",
    "<br>\n",
    "\n",
    "\n",
    "```python\n",
    "# Beispiel für Z-Score:\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "df['z_score'] = stats.zscore(df['column_name'])\n",
    "df_cleaned = df[(df['z_score'] > -3) & (df['z_score'] < 3)]  # Entfernt Ausreißer\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "## Validierung gegen externe Quellen\n",
    "\n",
    "Wenn möglich, sollten Daten gegen vertrauenswürdige externe Quellen validiert werden, um die Richtigkeit zu überprüfen. \n",
    "Dies ist besonders wichtig in kritischen Bereichen wie Finanzdaten oder medizinischen Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9ca63-6b87-4928-bcde-b62ff9368abe",
   "metadata": {},
   "source": [
    "---\n",
    "# Umgang mit irreführenden Informationen\n",
    "Irreführende Informationen können auftreten, wenn Daten zwar korrekt sind, aber falsch interpretiert werden oder nicht die gesamte Realität widerspiegeln.\n",
    "\n",
    "## Prüfung auf Korrelationen\n",
    "Manchmal können Variablen scheinbar in Beziehung zueinander stehen, obwohl es sich um zufällige oder irreführende Zusammenhänge handelt. Es ist wichtig, Korrelationen sorgfältig zu prüfen und nicht automatisch Kausalität anzunehmen.\n",
    "\n",
    "- Beispiel für Korrelationsanalyse:\n",
    "```python\n",
    "correlation_matrix = df.corr()\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "## Datenkontext verstehen\n",
    "Der Kontext der Daten ist entscheidend, um irreführende Schlüsse zu vermeiden. <br>\n",
    "Das bedeutet, dass die Datenquelle, der Erhebungsprozess und die Zielsetzung der Analyse verstanden werden müssen, um Fehlinterpretationen zu vermeiden.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Erkennen von Bias\n",
    "Verzerrungen (Bias) in den Daten, z.B. durch nicht repräsentative Stichproben, können zu irreführenden Ergebnissen führen. Es ist wichtig, potenzielle Verzerrungen zu erkennen und, wenn möglich, zu korrigieren. <br>\n",
    "Eine Möglichkeit ist die Gewichtung von Daten, um Verzerrungen auszugleichen.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Überprüfen der Datenquellen\n",
    "Wenn es Zweifel an der Verlässlichkeit der Datenquelle gibt:\n",
    "Datenquellen sollten auf ihre Genauigkeit und Verlässlichkeit überprüft werden. Es ist wichtig, sicherzustellen, dass Daten aus **verlässlichen und vertrauenswürdigen Quellen** stammen, um irreführende Ergebnisse zu vermeiden.\n",
    "\n",
    "- **Vorteile**: <br>\n",
    "  sichert die Integrität der Analyse, indem nur qualitativ hochwertige Daten verwendet werden\n",
    "- **Nachteile**: <br>\n",
    "  Validierung von Datenquellen kann zeitaufwendig sein\n",
    "\n",
    "<br>\n",
    "\n",
    "## Konsistenzprüfung\n",
    "Wenn Daten aus mehreren Quellen zusammengeführt werden oder wenn bestimmte logische Konsistenzen erwartet werden.\n",
    "\n",
    "- **Logische Konsistenzprüfungen**: <br>\n",
    "  z.B. sollte das Geburtsdatum vor dem Eintrittsdatum in eine Firma liegen\n",
    "- **Überprüfung auf doppelte oder widersprüchliche Werte**: <br>\n",
    "  Mehrfacheinträge oder widersprüchliche Angaben in den Daten können mithilfe von **deduplizierenden Algorithmen** und Vergleichstechniken aufgedeckt werden\n",
    "- **Vorteile**: <br>\n",
    "  reduziert Risiko von Fehlinterpretationen durch inkonsistente oder widersprüchliche Daten\n",
    "- **Nachteile**: <br>\n",
    "  kann komplex und zeitaufwendig sein, wenn viele Datenquellen oder Variablen betroffen sind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a973410-f162-4115-a418-fe63a3614457",
   "metadata": {},
   "source": [
    "---\n",
    "# Datenvalidierung\n",
    "Eine gute Praxis beim Umgang mit Daten ist die kontinuierliche Validierung während der Datenbereinigung. Dies kann durch automatisierte Prüfungen (z.B. durch Unit-Tests) oder durch stichprobenartige manuelle Überprüfungen geschehen.\n",
    "\n",
    "   **Beispiele für Validierungsprüfungen:**\n",
    "   - **Datenformat-Prüfungen:** <br>\n",
    "     Sicherstellen, dass Daten den erwarteten Datentypen entsprechen (z.B. Zahlen, Datumsangaben).\n",
    "   - **Prüfung auf logische Konsistenz:** <br>\n",
    "     Sicherstellen, dass Daten keine logischen Widersprüche enthalten (z.B. Geburtsdatum kann nicht in der Zukunft liegen).\n",
    "   - **Grenzwerte und Toleranzen:** <br>\n",
    "     Festlegen von zulässigen Datenbereichen, um fehlerhafte Eingaben zu erkennen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a2f686-de41-4886-836f-362d2d681e79",
   "metadata": {},
   "source": [
    "---\n",
    "# Automatisierung des Prozesses\n",
    "Manuelle Datenbereinigung ist in der Regel zeitaufwändig und fehleranfällig. Automatisierte Tools und Algorithmen können den Prozess beschleunigen und gleichzeitig die Genauigkeit erhöhen.\n",
    "   \n",
    "   **Beispiele:**\n",
    "   - **Pandas:** Für die Bearbeitung und Bereinigung von Datensätzen.\n",
    "   - **Scikit-learn:** Bietet Tools zur Imputation von fehlenden Werten und Erkennung von Ausreißern.\n",
    "   - **Data Validation Pipelines:** Um kontinuierliche Datenqualität sicherzustellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47102127-e1c2-4435-b4c3-c16f22156058",
   "metadata": {},
   "source": [
    "# Beispiel in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1d886c-5ef0-4193-9840-1ce0270f4c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bereinigte Daten:\n",
      "   customer_id  sales_amount  sales_date\n",
      "0          101           200  2023-01-01\n",
      "1          102           280  2023-01-02\n",
      "2          103           150  2023-01-03\n",
      "3          104           300  2023-01-04\n",
      "4          105           280  2023-01-03\n",
      "6          106           500  2023-01-07\n",
      "7          107           280  2023-01-03\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'customer_id': [101, 102, 103, 104, 105, 101, 106, 107],\n",
    "    'sales_amount': [200, 999, 150, 300, 999, 250, 500, 999],\n",
    "    'sales_date': ['2023-01-01', '2023-01-02', None, '2023-01-04', None, '2023-01-06', '2023-01-07', None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "#df_new = df.copy()\n",
    "\n",
    "#df[df['sales_amount]!=999].mean()\n",
    "\n",
    "\n",
    "# Fehlende Daten durch den Mittelwert der Spalte ersetzen\n",
    "df['sales_amount'] = df['sales_amount'].replace(999, df[df['sales_amount']!=999]['sales_amount'].mean())  # Der 999 ist nicht im Mittelwert berücksichtigt!!!\n",
    "\n",
    "# Fehlende Daten in der Spalte 'sales_date' mit einem Standardwert auffüllen\n",
    "df['sales_date'] = df['sales_date'].fillna('2023-01-03') \n",
    "# Duplikate entfernen\n",
    "df = df.drop_duplicates(subset='customer_id')\n",
    "\n",
    "# Bereinigte Daten anzeigen\n",
    "print(\"\\nBereinigte Daten:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d53d5f-bfbc-4312-8b48-068c8cd41012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
