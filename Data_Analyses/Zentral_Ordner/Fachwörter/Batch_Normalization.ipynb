{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c8af618-9abe-4f06-8424-42c615264232",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c263ec-f961-4f3b-a28f-a013f6ce5cc9",
   "metadata": {},
   "source": [
    "**Batch Normalization** ist eine Technik, die den Trainingsprozess neuronaler Netze stabilisiert und beschleunigt. Dabei werden die Eingaben jeder Schicht während des Trainings normalisiert (d.h. auf einen ähnlichen Wertebereich gebracht). Dies wird für Mini-Batches von Daten durchgeführt und reduziert das Risiko, dass das Netzwerk in lokale Minima fällt oder das [**Vanishing/Exploding Gradient Problem**](GP.ipynb) auftritt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280b48b-78c1-4b33-8574-fb1d049e35c4",
   "metadata": {},
   "source": [
    "### Hauptvorteile:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262730d3-0ebb-49b5-8f4f-6a3fd6d7299a",
   "metadata": {},
   "source": [
    "1. **Schnelleres Training**: Konvergenz wird beschleunigt.\n",
    "2. **Stabilerer Lernprozess**: Reduziert die Sensitivität gegenüber der Wahl der Initialgewichte.\n",
    "3. **Reduziert Überanpassung**: Durch die Regularisierungseffekte wird Overfitting verringert.\n",
    "\n",
    "Es führt zu konsistenteren und effektiveren Netzwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebbff4-3d69-49d6-8340-e5b144e39788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
