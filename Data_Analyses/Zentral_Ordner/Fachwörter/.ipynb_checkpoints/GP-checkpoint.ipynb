{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba3de07-9b4c-4972-981e-6abeaabdee5a",
   "metadata": {},
   "source": [
    "# Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db5efa-8c76-4001-8f1a-4ef5682ae91c",
   "metadata": {},
   "source": [
    "Das **Vanishing Gradient Problem** tritt bei tiefen neuronalen Netzen, insbesondere in **[Recurrent Neural Networks (RNNs)](RNN.ipynb)** und **[Deep Feedforward Networks](DFN.ipynb)**, auf. Dabei werden die **Gradienten** (Ableitungen), die für das Anpassen der Gewichte während des Trainings verwendet werden, in den frühen Schichten sehr klein. Dadurch lernen die vorderen Schichten kaum, da die Gewichte nur minimal aktualisiert werden. Dies verhindert das effektive Training tiefer Netze. \n",
    "\n",
    "Lösungen umfassen **[LSTM](LSTM.ipynb)**, **[Batch Normalization](Batch_Normalization.ipynb)** oder [**ResNet**-Architekturen](ResNet.ipynb), um das Problem zu umgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff5342f-7f83-498d-a5d2-780a64845a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
