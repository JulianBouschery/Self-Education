{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9d6e6f-2da8-411b-a8bf-d3d8fcdb09a6",
   "metadata": {},
   "source": [
    "# Recurrent Neral Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d925bef2-3a58-4755-ac3f-84215d80f387",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) sind eine Klasse von neuronalen Netzwerken, die speziell für die Verarbeitung sequenzieller Daten entwickelt wurden. Sie sind in der Lage, Informationen aus vorherigen Zeitschritten zu speichern und zu nutzen, was sie besonders nützlich für Aufgaben wie Zeitreihenanalyse, Sprachverarbeitung und Textgenerierung macht. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e5e449-2a48-47ca-b5b1-a66837c59c95",
   "metadata": {},
   "source": [
    "### Wichtige Merkmale von RNNs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a47559-bf94-43a6-8d1c-ffadbfc730fd",
   "metadata": {},
   "source": [
    "1. **Verborgene Zustände**: RNNs verwenden versteckte Zustände (hidden states), die Informationen über vorherige Eingaben speichern. Diese Zustände werden bei jedem Zeitschritt aktualisiert, wodurch das Netzwerk ein Gedächtnis für vergangene Eingaben hat.\n",
    "\n",
    "2. **Schleifenstruktur**: Im Gegensatz zu Feedforward-Netzwerken besitzen RNNs Rückkopplungsschleifen, die es ermöglichen, dass Ausgaben wieder als Eingaben in das Netzwerk zurückgeführt werden. Dies ist entscheidend für die Verarbeitung von Sequenzen.\n",
    "\n",
    "3. **Anwendung**: RNNs werden häufig für Anwendungen verwendet, bei denen der zeitliche Zusammenhang von Bedeutung ist, wie z.B. bei der Sprachsynthese, maschinellen Übersetzung, Textklassifikation und Musikgenerierung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df06e1a-9ef8-47a8-85f4-7b1b83870051",
   "metadata": {},
   "source": [
    "### Herausforderungen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c35e1-7351-4c6b-b21d-08df3e4166f3",
   "metadata": {},
   "source": [
    "- **Vanishing Gradient Problem**: RNNs leiden oft unter dem Vanishing Gradient Problem, bei dem die Gradienten in langen Sequenzen sehr klein werden, was das Training erschwert. \n",
    "\n",
    "- **Exploding Gradient Problem**: Sie können auch unter dem Exploding Gradient Problem leiden, bei dem die Gradienten während des Trainings exponentiell anwachsen und das Modell instabil machen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e0ae2b-7a18-43ce-a852-7a2400156f8d",
   "metadata": {},
   "source": [
    "### Erweiterungen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554270e3-318b-4580-ad5e-1df8f3920652",
   "metadata": {},
   "source": [
    "Um diese Probleme zu adressieren, wurden spezielle Architekturen wie Long Short-Term Memory (LSTM) und Gated Recurrent Units (GRU) entwickelt. Diese Varianten enthalten Mechanismen, die das Gedächtnis über längere Zeiträume erhalten und die Effizienz des Lernens verbessern.\n",
    "\n",
    "Insgesamt sind RNNs ein leistungsstarkes Werkzeug für die Verarbeitung von sequenziellen Daten, erfordern jedoch oft fortgeschrittene Techniken zur Stabilisierung des Trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15992d-e1d3-4ee9-87e4-dd77cf7f971e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
